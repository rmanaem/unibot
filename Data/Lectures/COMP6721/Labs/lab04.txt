COMP 6721 Applied Artiﬁcial Intelligence (Winter 2022) Lab Exercise #4: Naïve Bayes Classiﬁcation Question 1 Assume that a fancy food-store sells wild hand-picked mushrooms from a local farmer. In the store, the mushrooms are labelled as gourmet, good, or at- your-own-risk. The store always keeps the following inventory: 25% of its mushrooms are labeled gourmet, 50% are labeled good, and 25% are labeled at-your-own-risk. Mushrooms labeled as gourmet have a 5% chance of being poisonous, a good mushroom has a 15% chance of poisoning someone, and a at-your-own-risk mushroom has a 25% chance. If Jim bought a mushroom from the store and was poisoned, (a) What is the probability that the mushroom had been labeled gourmet? (b) What is the probability that the mushroom had been labeled good? (c) What is the probability that the mushroom had been labeled at-your-own- risk? 1 BreakPage Question 2 Assume that Cecilia receives many e-mails from her home town in Klinga, where people speak Klinish. If you do not know Klinish, don’t worry. It is a simple language made up of only 1,000 words that all start with the letter “k”. A Klinish document may also contain words that do not start with “k”, but these are considered out-of-vocabulary words (like a proper name, for example). Jack is trying to help Cecilia sort her Inbox into 3 mail folders (Personal, Work and Promotion). However, Jack does not speak Klinish, so all he has to work from are old e-mails that Cecilia has already sorted into the right folders. The table below shows a sample of the data that Jack has gathered from Cecilia’s previous e-mails. The table indicates the frequency of each Klinish word in each folder (to be complete, the table should contain 1,000 rows, corresponding to each word in Klinish). For example, the word kiki appeared 30 times in e-mails labelled Personal, 50 times in e-mails about Work, and 9 times in Promotion e-mails. Folder Personal Work Promotion kami 45 12 17 kawa 78 1 67 keke 0 5 80 kiki 30 50 9 Word koko 6 10 10 kotuku 5 27 20 koula 17 56 3 ... Total Nb of Words 20,000 25,000 17,000 The table above corresponds to data collected from 50 e-mails labeled Per- sonal, 65 e-mails labeled Work and 45 e-mails labeled Promotion. Based on the data above, Jack is trying to classify the following two e-mails (note that upper and lower cases should not be distinguished). Email 1: Koko kami kawa koula keke Email 2: Keke kawa, koko Google koula keke! (a) Use a Naïve Bayes classiﬁer without any smoothing, to classify the two e-mails above. Use the sum of logs (base 10), and show the score of each of the 3 classes (Personal, Work and Promotion) and the most likely class. (b) Do the same as part A above, but this time use “add 0.5 smoothing” (i.e., instead of adding the value 1 to each word frequency, add ½ to each word 2 BreakPage frequency). Adjust the smoothing formula accordingly, and show all your work. Again, use the sum of logs (base 10), and show the score of each of the 3 classes and the most likely class. 3 BreakPage Question 3 Let’s write a Python program to train and run a model using the Multinomial Naïve Bayes Classiﬁer. You can use the implementation provided with scikit- learn:1 import numpy as np from sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer Start by implementing the Email Spam Classiﬁer you’ve worked through on Worksheet #3. Create the training data: corpus = np.array([ 'cheap meds for sale', 'click here for the best meds', 'book your trip', 'cheap book sale, not meds', 'here is the book for you' ]) To transform the text corpus into a feature vector, you can use scikit-learn’s CountVectorizer :2 vectorizer = CountVectorizer() X = vectorizer.fit_transform(corpus) You also need the target vector with the labels for the training data (here, spam is 0 and ham is 1): y = np.array([0,0,0,1,1]) Get a classiﬁer using the prior probabilities for each class (0.6 for spam, 0.4 for ham): classifier = MultinomialNB(class_prior=[0.6, 0.4]) Train a model using your classiﬁer: model = classifier.fit(X, y) Now you can try to apply your model to classify a new email as SPAM or HAM. Here is the example email (’the cheap book’) as a feature vector: new_mail = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]] (a) Create a complete, working Python program. Print out the intermediate variables to see the data you are working with. Predict the class for the new_mail using your model and print it out. 1See https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html 2See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text. CountVectorizer.html 4 BreakPage (b) Inspect the scikit-learn documentation to understand how smoothing is handled for this algorithm. (c) Change the code to transform the new_mail automatically from a string into a feature vector. (d) Change the code to automatically compute the prior probabilities using the training data. Print out the priors for the model to verify that they are indeed correct. 5 BreakPage 