COMP 6721 Applied Artiﬁcial Intelligence (Winter 2022) Lab Exercise #1: Overview and History of AI Question 1. Google these events on the history of AI and sort them in chronological order, indicating the year that they occurred if you can. Feel free to browse the Web and read here and there about AI. 1) Expert systems, such as MYCIN, and other types of systems made of hand-written rules are considered too expensive to maintain and to adapt to new domains. The industry drops research in such systems. It is the 2nd AI Winter. 2) While at the Cornell Aeronautical Laboratory, Frank Rosenblatt develops the perceptron, the ﬁrst artiﬁcial neuron, that will set the stage for the development of networks of artiﬁcial neurons, a.k.a. neural networks. 3) The Association for Computing Machinery (ACM) names Yoshua Bengio (from MILA, Montreal), Geoﬀrey Hinton (from Vector Institute, Toronto), and Yann LeCun (from Facebook) recipients of the Turing Award for their contribution in the ﬁeld of AI and Deep Learning. 4) Marvin Minsky & Seymour Papert publish a book that shows the limits of perceptrons and argue for more work in symbolic computation (aka Good-Old-Fashioned AI, GOFAI). The book is often cited as the main reason for the abandoning research on neural networks. 5) The Lighthill and the ALPAC reports that showed little progress in AI, kills research funding and leads to the ﬁrst AI Winter. 6) AlphaGo beats the world’s champion at the game of Go. AlphaGo’s strategy is learned automatically from playing a large number of games against itself. 7) For his PhD project, Terry Winograd, develops a system called SHRDLU, that under- stands English instructions. SHRDLU is based on hand-written rules. 8) The robot Shakey1, programmed in LISP, resulted in the development of the A* search algorithm. 9) The AlexNet system, developed at the University of Toronto by Alex Krizhevsky, a PhD student of Geoﬀrey Hinton, wins the ImageNet Challenge and shows that deep learning techniques can achieve signiﬁcantly better results than classical machine learning tech- niques in image processing. This is considered to be a deﬁning moment in the history of AI. 1Guess why it was called this way? ;-) 1 BreakPage 10) Marvin Minsky develops the Frames to reason with world-knowledge. Years later, frames turned out to be the basis of object-oriented programming. 11) The expert system MYCIN is developed to recognise bacterial infections and recommend antibiotics. Its recommendations are often better than those of human experts. It is based on a knowledge base of ≈ 600 hand-written rules (written in Lisp) and developed in collaboration with medical doctors. 12) Joseph Weizenbaum develops Eliza, the ﬁrst chatbot. It is based on a set of re-write rules written by hand. 13) Corinna Cortes and Vladimir Vapnik develop an approach to machine learning called soft margin Support Vector Machines (SVM), which quickly becomes one of the most popular machine learning algorithm. 14) The term “Artiﬁcial Intelligence” is ﬁrst used. 15) A Portrait of Edmond Belamy created by a Generative Adversarial Network (GAN) sells for $432,500, at Christie’s auction in New York city. 16) After ﬁnishing his PhD on handwriting recognition, Yann Lecun makes public the MNIST dataset. The dataset contains 70,000 images of handwritten digits and becomes the benchmark to evaluate machine learning. 17) Alain Colmerauer, who was professor at the University of Montreal for a few years, develops Prolog; a programming language based on logics that is very popular in AI to write rule-based systems. 18) Google launches its Google Translate service based on Statistical Machine Translation. Translation rules are found automatically based on a statistical analysis of parallel texts in diﬀerent languages. 19) While having a beer at Les 3 Brasseurs at the corner of McGill College and Ste-Catherine, Ian Goodfellow, a PhD student of Yoshua Bengio, comes up with an idea for a neural net- work that could generate realistic images. He called it Generative Adversarial Networks, a.k.a. GANs. 20) The METEO rule-based machine translation system, developed at the University of Mon- treal, is deployed at Environment Canada to translate weather forecasts from English to French. 21) Google launches its Neural Machine Translation system based on recent advances in Deep Neural Networks, and little by little drops the Statistical Machine Translation approach of the years 2000’s. 22) Canada invests massively in 3 AI research institutes: Amii in Alberta, Mila in Montréal, and Vector Institute in Toronto. 2 BreakPage 23) OpenAI develops GPT and GPT-2, two language models that can predict the next word in a text given the previous ones. GPT-2 is so powerful compared to previous models, that it can generate fake stories in English that are somewhat coherent. 24) Netﬂix initiates a competition in machine learning to beat its own ﬁlm recommendation system. It provides a data set of about 100 millions movie ratings to learn recommenda- tions automatically. 25) Alan Turing develops the Turing Test. 3 BreakPage 