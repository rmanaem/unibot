COMP 6721 Applied Artiﬁcial Intelligence (Winter 2022) Worksheet #5: Neural Networks Perceptron. Calculate your ﬁrst neuron activation for the Perceptron (only 100 billion−1 more to go!): Activation function: (cid:40) 1, if (cid:126)x · w(cid:126) ≥ threshold f ((cid:126)x) = 0, otherwise (use a threshold of 0.55): f ((cid:126)x) = Perceptron Learning. Ok, so for the ﬁrst training example, the perceptron did not produce the right output. To learn the correct result, it has to adjust the weights: ∆w = η(T − O), where we set η = 0.05 (our learning rate). T is the expected output and O the output produced by the perceptron. Remember to only update weights for active connections (i.e., with non-zero input): Student w w w w f ((cid:126)x) ok? 1 2 3 4 Richard 0.2 0.2 0.2 0.2 Alan Alison Delta Rule. In the generalized delta rule for training the perceptron, we add a bias input that is always one and has its own weight (here w ). Weight changes ∆w now take the input value x into account. We 3 i i want the perceptron to learn the two-dimensional data shown on the right: Data w w w f ((cid:126)x) ok? Assume we use the sign function with a threshold 1 2 3 of 0 and a learning rate η = 0.2. The weights #1 0.75 0.5 -0.6 are initialized randomly as shown in the table. #2 Apply the generalized delta rule for updating the #3 weights: ∆w = η(T − O)x i i #4 BreakPage COMP 6721 Worksheet: Neural Networks Winter 2022 Neural Network for XOR. To learn a non-linearly separable function like XOR, we’ll use a neural network with a hidden layer (the weights have been initialized randomly): x x x XOR x 1 2 1 2 1 1 0 0 0 0 1 0 1 0 1 1   (cid:88) Oi = sigmoid  wjixj j 1 = 1 + e−((cid:80)j wjixj) Step 1. Compute the output for the three neurons O , O and O for the input (x = 1, x = 1): 3 4 5 1 2 O = O = O = 3 4 5 Step 2. The next step is to calculate the error δ ← g(cid:48)(x ) × Err = O (1 − O ) × (O − T ) k k k k k k k starting from the output neuron O : δ = O (1−O )×(O −T ) = 5 5 5 5 5 5 Step 3. Now we calculate the error terms for the hidden layer: (cid:88) δ ← g(cid:48)(x ) × Err = O (1 − O ) × w δ h h h k k kh k k∈outputs For the two neurons (3), (4) in the hidden layer: • δ = O (1 − O )δ w = 3 3 3 5 35 • δ = O (1 − O )δ w = 4 4 4 5 45 Step 4. Now we can update our weights using w ← w + ∆w , where ∆w = −ηδ x ij ij ij ij j i Here, we assume a constant learning rate η = 0.1: • ∆w = 14 • ∆w = 24 • ∆w = 45 • ∆Θ = 5 And ﬁnally update the weights (w ← w + ∆w ): ij ij ij • w = w + ∆w = 14 14 14 • w = w + ∆w = 24 24 24 • w = w + ∆w = 45 45 45 • Θ = Θ + ∆Θ = 5 5 5 BreakPage 