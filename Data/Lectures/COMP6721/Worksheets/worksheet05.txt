

























































COMP 6721 Applied Artificial Intelligence (Winter 2022)

Worksheet #5: Neural Networks

Perceptron. Calculate your first neuron activation for the Perceptron (only 100 billion−1 more to go!):

Activation function:

f(~x) =

{
1, if ~x · ~w ≥ threshold
0, otherwise

(use a threshold of 0.55):
f(~x) =

Perceptron Learning. Ok, so for the first training example, the perceptron did not produce the right
output. To learn the correct result, it has to adjust the weights: ∆w = η(T −O), where we set η = 0.05
(our learning rate). T is the expected output and O the output produced by the perceptron. Remember to
only update weights for active connections (i.e., with non-zero input):

Student w1 w2 w3 w4 f(~x) ok?

Richard 0.2 0.2 0.2 0.2

Alan

Alison

Delta Rule. In the generalized delta rule for training the perceptron, we add a bias input that is always
one and has its own weight (here w3). Weight changes ∆wi now take the input value xi into account. We
want the perceptron to learn the two-dimensional data shown on the right:

Assume we use the sign function with a threshold
of 0 and a learning rate η = 0.2. The weights
are initialized randomly as shown in the table.
Apply the generalized delta rule for updating the
weights: ∆wi = η(T −O)xi

Data w1 w2 w3 f(~x) ok?

#1 0.75 0.5 -0.6

#2

#3

#4



COMP6721 Worksheet: Neural Networks Winter 2022

Neural Network for XOR. To learn a non-linearly separable function like XOR, we’ll use a neural network
with a hidden layer (the weights have been initialized randomly):

x1 x2 x1 XOR x2

1 1 0

0 0 0

1 0 1

0 1 1

Oi = sigmoid


∑

j

wjixj




=
1

1 + e−(
∑

j wjixj)

Step 1. Compute the output for the three neurons O3, O4 and O5 for the input (x1 = 1, x2 = 1):
O3 = O4 = O5 =

Step 2. The next step is to calculate the error

δk ← g′(xk)× Errk = Ok(1−Ok)× (Ok − Tk)

starting from the output neuronO5: δ5 = O5(1−O5)×(O5−T5) =

Step 3. Now we calculate the error terms for the hidden layer:

δh ← g′(xh)× Errh = Ok(1−Ok)×
∑

k∈outputs
wkhδk

For the two neurons (3), (4) in the hidden layer:

• δ3 = O3(1−O3)δ5w35 =

• δ4 = O4(1−O4)δ5w45 =

Step 4. Now we can update our weights using

wij ← wij + ∆wij , where ∆wij = −ηδjxi

Here, we assume a constant learning rate η = 0.1:

• ∆w14 =

• ∆w24 =

• ∆w45 =

• ∆Θ5 =

And finally update the weights (wij ← wij + ∆wij):

• w14 = w14 + ∆w14 =

• w24 = w24 + ∆w24 =

• w45 = w45 + ∆w45 =

• Θ5 = Θ5 + ∆Θ5 =


