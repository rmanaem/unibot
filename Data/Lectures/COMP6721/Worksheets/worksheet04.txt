COMP 6721 Applied Artiﬁcial Intelligence (Winter 2022) Worksheet #4: Decision Trees & k-means Clustering Decision Tree. Given the following training data: Create a decision tree that decides if a student will get an ‘A’ this year, based on an input feature vector X. (Note: check that your tree would return the correct answer for all of the training data above.) Your Decision Tree Information Content. The information content of an event x with P (x) > 0 is deﬁned as: −P (x) · log (P (x)) 2 An impossible event (P (x) = 0) is deﬁned as having an information content of 0. What’s the information content of a certain event (P (x) = 1)? BreakPage COMP 6721 Worksheet: Decision Trees & k-means Clustering Winter 2022 Entropy. Using the deﬁnition of Entropy for a discrete random variable X with possible outcomes x , x , . . . , x : 1 2 n n (cid:88) H(X) = − p(x ) · log p(x ) i 2 i i=1 compute the entropy for the outcome of the color in Roulette, where you have the numbers 1–36 (half red, half black) and the 0 with the color green: H(X) = Note: make sure you use log (x); if you have a calculator with log only, you can compute it using the formula 2 10 log (x) = log (x)/ log (2). 2 10 10 Information Gain. Compute the Information Gain (IG) for the following training data when splitting using the “Size” attribute: gain(S, A) = H(S) − H(S|A) = H(S) − (cid:88) |Sv| · H(S ) |S| v v∈values(A) H(S|Size) = gain(Size) = H(S) − H(S|Size) = F-Measure. Compute the F-Measure, which combines precision and recall into a single number, using β = 1 (called F -measure): 1 2 · P · R F = 1 P + R For the systems from the previous lecture worksheet: 1. s : P = 100%, R = 60% ⇒ F = 2 1 2. s : P = 71%, R = 100% ⇒ F = 3 1 k-Means Clustering. Here is a dataset with two attributes, to be grouped into two clusters. Compute the distance d(p(cid:126), (cid:126)q) = (cid:112)(cid:80)n (p − q )2 of each data point to the two initial centroids and assign each point to its i=1 i i closest cluster: a1 a2 Distance to C1 Distance to C2 Cluster Centroid Data1 1.5 2.0 a1 a2 Data2 3.0 4.0 Cluster 1 1.0 1.0 Data3 4.5 5.0 Cluster 2 5.0 7.0 Data4 3.5 4.5 BreakPage 