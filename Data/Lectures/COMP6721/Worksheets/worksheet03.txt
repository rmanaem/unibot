COMP 6721 Applied Artiﬁcial Intelligence (Winter 2022) Worksheet #3: Na¨ıve Bayes Classiﬁer Joint Probabilities. Given the following joint probability distribution: Compute the probability that someone has a cavity, given a toothache: P (cavity|toothache) = Bayes’ Theorem. Assume students come to the lecture either by car (event A) or by metro. Event B means the student arrives on-time for the lecture. One student uses the car 70% of the time, i.e., P (car) = P (A) = 0.7. In this case, the student is 80% on-time, i.e., P (ontime|car) = P (B|A) = 0.8. Also, this student is on-time in general in 60% of all cases, i.e., P (ontime) = P (B) = 0.6. Today the student arrived on time. How likely is it that this student came by car? Apply Bayes’ Theorem: P (B|A) · P (A) P (A|B) = = P (B) AI Fraud Detection. You just built your ﬁrst AI system for detecting fraudulent credit card transactions (event B). In your company, 0.01% of all transactions are fraudulent, i.e., P (B) = 0.0001. Event A is “system detected fraud”. You tested your system with existing training data and determined that it ﬁnds fraudulent cases with a 96% success rate, i.e., P (A|B) = 0.96. Unfortunately, it also sounds an alarm in 1% of non-fraudulent cases, i.e., P (A|B) = 0.01 (B is the complement of B). So, when your system sounds the fraud alarm, in how many percent of the cases was it a false alarm? Hint: You will need P (A), which you can compute using P (A) = P (A|B) · P (B) + P (A|B) · P (B). AI Weather Prediction. Now we can build a weather-predicting AI using Bayes’ theorem: Today we observe an average sunset (E ). What kind of weather will we have tomorrow? Compute the probabilities 2 for each hypothesis (H , H , H ) using 1 2 3 P (H ) · P (E |H ) P (H |E ) = i 2 i , with P (E ) = P (H ) · P (E |H ) + P (H ) · P (E |H ) + P (H ) · P (E |H ) = 0.31 i 2 P (E ) 2 1 2 1 2 2 2 3 2 3 2 1. P (H |E ) = 1 2 2. P (H |E ) = 2 2 3. P (H |E ) = 3 2 So, tomorrow’s weather will be: BreakPage COMP 6721 Worksheet: Na¨ıve Bayes Classiﬁer Winter 2022 Email Spam Detector. Let’s train an email spam detector using a Multinomial Na¨ıve Bayes Classiﬁer, so it can classify future emails for you into the classes spam & ham. Here is your training data: c : SPAM documents: c : HAM documents: 1 2 • d : “cheap meds for sale” • d : “cheap book sale, not meds” 1 4 • d : “click here for the best meds” • d : “here is the book for you” 2 5 • d : “book your trip” 3 1. Record the count of each word per class below. Ignore words from the documents that are not in the table: best book cheap sale trip meds #words c : SPAM 1 c : HAM 2 2. Now compute the conditional probabilities P (w |c ) for each word/class, as well as the prior probability P (c ) j i i for each class, based on your training data: best book cheap sale trip meds P (c ) i c : SPAM 1 c : HAM 2 3. Now you have a new email coming in: • d : “the cheap book” 6 Is this email spam or ham? Apply Bayes’ Algorithm to ﬁnd out which class has a higher probability: (a) P (c ) = 1 (b) P (c ) = 2 So, the new email is: Machine Learning System Evaluation. Consider the results from three diﬀerent ML systems on a binary classiﬁcation task. Here, X1–X5 are the instances that the systems should have recognized as belonging to a speciﬁc class (e.g., spam email, cat photo, fraud transaction). The remaining 495 instances do not belong to this class: Evaluate the performance of the three systems using the measures Accuracy, Precision, and Recall : system 1 system 2 system 3 Accuracy Precision Recall BreakPage 