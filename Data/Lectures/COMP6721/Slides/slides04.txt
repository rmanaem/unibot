Artificial Intelligence:  Naive Bayes Classification 1 BreakPage Remember this slide… 2 BreakPage Today Introduction to ML 1. Naïve Bayes Classifier 2. Evaluation 3. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 3 BreakPage Why Machine Learning? Over 2.5 quintillion bytes of data are created every single day, and it is only going to  grow from there. By 2020, it is estimated that 1.7MB of data will be created every  second for every person on earth. 4 BreakPage Why Machine Learning? 5 BreakPage Machine Learning History In 1959, Arthur Samuel first proposed  the concept Machine Learning: “A computer program is said to learn from  experience E with respect to some class of   tasks T and performance measure P if its  performance at tasks in T, as measured by  P, improves with experience E.” 6 BreakPage Machine Learning Process 7 BreakPage Supervised Learning 8 BreakPage Unsupervised Learning 9 BreakPage Reinforcement Learning 10 BreakPage Types of ML Algorithms Machine Learning Supervised  Unsupervised  Reinforcement Learning Learning Learning (learns by reacting to  (task-driven) (data analytics) environment) Classification Regression Association Clustering Reward Based 11 BreakPage ML outside of AI ML is widely used in Data Mining  a.k.a. Knowledge Discovery in Databases (KDD)  e.g. Clustering, Anomaly Detection,  Association Rule Mining  Example: predict if a customer is likely to purchase  certain goods according to history of shopping  activities. 12 BreakPage Types of Machine Learning Supervised Unsupervised  Reinforcement  Learning  Learning Learning Definition The machine learns by  The machine is trained  An agent interacts with its  using labelled data on unlabeled data  environment by producing  without any guidance actions & discovers errors  and rewards Types of problems Regression &Classification Association & Clustering Reward based Type of data Labelled data Unlabelled data No pre-defined data Training External supervision No supervision No supervision Approach Map labelled input to  Understand patterns and  Follow trail and error  known output discover output method Popular  Linear Regression,  K-means, C-means, etc Q-learning, etc Algorithms Logistic Regression, KNN,  etc 13 BreakPage Types of ML Problems 14 BreakPage Today Introduction to ML 1. Naïve Bayes Classifier 2. Evaluation 3. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 15 BreakPage Motivation  How do we represent and reason about  non-factual knowledge?  It might rain tonight  If you have red spots on your face, you might have  the measles  This e-mail is most likely spam  I can’t read this character, but it looks like a “B”  These 2 pictures are very likely of the same  person  … 16 BreakPage Remember…  P is a probability function:  0 ≤ P(A) ≤ 1  P(A) = 0 ⇒ the event A will never take place  P(A) = 1 ⇒ the event A must take place A A∩B B  ∑  P(A ) = 1 ⇒ one of the events A  will take place i i i  P(A) + P(~A) = 1  Joint probability  intersection A ∩ ...∩ A  is an event that takes place if all the events  1    n A ,...,A  take place 1 n  denoted P(A∩B) or P(A,B)   Sum Rule  union A ∪...∪ A  is an event that takes place if at least one of the  1 n events A ,...,A  takes place 1 n  denoted P(A ∪ B) = P(A) + P(B) - P(A ∩ B) 17   BreakPage Conditional Probability  Prior (or unconditional) probability  Probability of an event before any evidence is  obtained  P(A)  = 0.1 P(rain today) = 0.1  i.e. Your belief about A given that you have no  evidence  Posterior (or conditional) probability  Probability of an event given that you know that B is  true       (B = some evidence)  P(A|B) = 0.8 P(rain today| cloudy) = 0.8  i.e. Your belief about A given that you know B 18 BreakPage Conditional Probability (con’t) P(A  B)  P(A, B) P(A | B)     P(B) P(B) A B A∩B 19 BreakPage Example  Rolling two dice (together):  Rolling two dice one after the other, first dice rolled 1: 20 BreakPage Chain Rule  With 2 events, the probability that A and B occur is: P(A, B) P(A|B)     so  P(A, B)  P(A|B) x P(B) P(B)  With 3 events, the probability that A, B and C occur is:  The probability that A occurs    Times, the probability that B occurs, assuming that A occurred      Times, the probability that C occurs, assuming that A and B have    occurred  With n events, we can generalize to the Chain rule: P(A , A , A , A , ..., A )  1 2 3 4 n = P (∩A ) i = P(A ) × P(A |A ) × P(A |A ,A ) × ... × P(A |A ,A ,A ,…,A ) 1 2 1 3 1 2 n 1 2 3 n-1 21 BreakPage So what?  we can do probabilistic inference  i.e. infer new knowledge from observed evidence 22 BreakPage Example 1  Joint probability distribution: P(Toothache ∩ Cavity) evidence s Toothache ~Toothache i s e h Cavity 0.04 0.06 t o p y ~Cavity 0.01 0.89 h P( H  E) P(H | E)  P(E) → Worksheet #3 (“Joint Probabilities”) 24 BreakPage Getting the Probabilities  in most applications, you just count from a set of  observations count_of_A P(A)  count_of_a ll_events P(A  B) count_of_A _and_B_tog ether P(A | B)   P(B) count_of_a ll_B 25 BreakPage Combining Evidence  Assume now 2 pieces of evidence:  Suppose, we know that  P(Cavity | Toothache) = 0.12  P(Cavity | Young) = 0.18  A patient complains about Toothache and is Young…   what is P(Cavity | Toothache ∩ Young) ?  26 BreakPage Combining Evidence Toothache ~Toothache Young ~ Young Young ~ Young Cavity 0.108 0.012 0.072 0.008 ~Cavity 0.016 0.064 0.144 0.576 P(Toothache ∩ Cavity ∩ Young)  But how do we get the data ?  In reality, we may have dozens, hundreds of variables  We cannot have a table with the probability of all  possible combinations of variables  Ex. with 16 binary variables, we would need 216 entries 27 BreakPage Independent Events  In real life:  some variables are independent…   ex: living in Montreal & tossing a coin  P(Montreal, head) = P(Montreal) * P(head)  probability of 2 heads in a row:   P(head, head) = 1/2 * 1/2 = 1/4  some variables are not independent…  ex: living in Montreal & wearing boots  P(Montreal, boots) ≠ P(Montreal) * P(boots) 28 BreakPage Independent Events  Two events A and B are independent:  if the occurrence of one of them does not influence the  occurrence of the other  i.e.  A is independent of B if P(A) = P(A|B)  If A and B are independent, then:   P(A,B) = P(A|B) x P(B) (by chain rule)       = P(A)    x P(B) (by independence)  To make things work in real applications, we often assume  that events are independent   , P(A B) = P(A) x P(B) 29 BreakPage Conditional Independent Events  Two events A and B are conditionally  independent given C:  Given that C is true, then any evidence about B  cannot change our belief about A  P(A, B |  C) = P(A | C) x P(B | C).  30 BreakPage Bayes’ Theorem P(A, B)  given: P(A|B)     so  P(A, B)  P(A|B) x P(B) P(B) P(A, B) P(B | A)     so  P(A, B)  P(B | A) x P(A) P(A)  then: P(A|B) x P(B) = P(B|A) x P(A)    and: → Worksheet #3 (“Bayes’ Theorem”) 31 BreakPage So?  We typically want to know: P(Hypothesis | Evidence)  P(Disease | Symptoms)… P(meningitis | red spots)   P(Cause | Side Effect)… P(misaligned brakes| squeaky wheels)  But P(Hypothesis| Evidence) is hard to gather  ex: out of all people who have red spots… how many have meningitis?   However P(Evidence | Hypothesis) is easier to gather   ex: out of all people who have the meningitis … how many have red  spots?  So  P(Evidence | Hypothesis) P(Hypothesis) P(Hypothesis | Evidence)  P(Evidence ) 32 BreakPage Example 2 Assume we only have 1 hypothesis Assume:  P(spots=yes | meningitis=yes) = 0.4  P(meningitis=yes) = 0.00003  P(spots=yes) = 0.05 P(meningit is  yes | spots  yes) P(spots  yes | meningitis  yes) x P(meningit is  yes)  P(spots  yes) 0.4 0.00003  0.00024 0.05  If you have spots… you are more likely to have  → meningitis than if we don’t know about you having  spots → Worksheet #3 (“AI Fraud Detection”) 33 BreakPage Example 3  Predict the weather tomorrow based on tonight‘s sunset...  Assume we have 3 hypothesis...  H : weather will be nice P(H ) = 0.2 1 1  H : weather will be bad  P(H ) = 0.5  2 2  H : weather will be mixed  P(H ) = 0.3  3 3  And 1 piece of evidence with 3 possible values  E : today, there's a beautiful sunset 1 P(E |H ) 2  1  E : today, there's a average sunset 2  E : today, there's no sunset 3 P(E |H ) E E E x i 1 2 3 H 0.7 0.2 0.1 1 H 0.3 0.3 0.4 2 H 0.4 0.4 0.2 3 34 BreakPage Example 3  Observation: average sunset (E ) 2  Question: how will be the weather tomorrow?   P(H  | E ) ? i 2  predict the weather that maximizes the probability  select H  such that P(H  | E ) is the greatest i i 2 P(H ) x P(E | H ) i 2 i P(H | E )   i 2 P(E ) 2 P(E )  P(H ) x P(E | H )  P(H ) x P(E | H )  P(H ) x P(E | H ) 2 1 2 1 2 2 2 3 2 3  .2x.2  .5x.3  .3x.4  .04  .15  .12  0.31 → Worksheet #3 (“AI Weather Prediction”) 36 BreakPage Bayes’ Reasoning  Out of n hypothesis…  we want to find the most probable H  given the evidence E i  So we choose the H  with the largest P(H |E) i i  P(H ) x P(E |H )  H   argmax P(H | E)  argmax  i i NB i P(E) H H i i  But… P(E)   is the same for all possible H   (and is hard to gather anyways) i  so we can drop it   So Bayesian reasoning:  P(H ) x P(E | H )  H argmax   i i  argmax   P(H ) x P(E | H )  NB i i P(E) H H i i 38 BreakPage Representing the Evidence  The evidence is typically represented by many  attributes/features  beautiful sunset? clouds? temperature? summer?, …  so often represented as a feature/attribute vector     evidence    hypothesis    sunset  clouds   temp  summer    weather  a   a   a .   a   tomorrow  1 2 3 4 e1  beautiful  no  high  yes    Nice     e1 = <a , … , a > 1 n  e1 = <sunset:beautiful, clouds:no, temp:high, summer:yes> 39 BreakPage Combining Evidence    toothache  young  cavity  yes  yes  ?    P(Cavity  yes|Toothache  yes  Young  yes) ?   with Bayes Rule : P(Toothach e  yes  Young  yes| Cavity  yes)xP(Cavity  yes)    P(Toothach e  yes  Young  yes) with independen ce assumption : P(Toothach e  yes  Young  yes|Cavity  yes)xP(Cavity  yes)  P(Toothach e  yes ) x P(Young  yes ) with conditiona l independen ce assumption : P(Toothach e  yes|Cavity  yes )  xP(Young  yes|Cavity  yes)x P(Cavity  yes)  P(Toothach e  yes ) x P(Young  yes) Now we have decomposed the joint probability distribution into much  smaller pieces…  40 BreakPage   Combining Evidence  toothache  young  cavity  yes  yes  yes?  or no?    But since we only care about ranking the hypothesis… P(Cavity yes | Toot hache yes    Young yes)  P(Cavity no| Tootha che yes    Young yes)  P(Cavity yes) P(Toothach e yes| Cavity yes) P(Young yes| Cavity yes) P(Toothach e yes)   P(Young yes)  P(Cavity no) P(Toothach e yes| Cavity no) P(Young yes| Cavity no) P(Toothach e yes)  P(Young yes) P(Cavity yes) P(Toothach e yes| Cavity yes) P(Young yes| Cavity yes)  P(Cavity no) P(Toothach e yes| Cavity no) P(Young yes| Cavity no) P(H ) x P(E|H )  n H argmax  i i  argmax  P(H ) x P(E|H ) argmax  P(H ) x P( a , a , a ,..., a |H )  argmax  P(H ) x P(a |H )  NB P(E) i i i 1 2 3 n i i j i Hi Hi Hi Hi j1 41 BreakPage Example 4 evidence Day  Outlook  Temperature  Humidity  Wind  Play Tennis  Day1  Sunny  Hot  High  Weak  No  Day2  Sunny  Hot  High  Strong  No  Day3  Overcast  Hot  High  Weak  Yes  Day4  Rain  Mild  High  Weak  Yes  Day5  Rain  Cool  Normal  Weak  Yes  Day6  Rain  Cool  Normal  Strong  No  Day7  Overcast  Cool  Normal  Strong  Yes  Day8  Sunny  Mild  High  Weak  No  Day9  Sunny  Cool  Normal  Weak  Yes  Day10  Rain  Mild  Normal  Weak  Yes  Day11  Sunny  Mild  Normal  Strong  Yes  Day12  Overcast  Mild  High  Strong  Yes  Day13  Overcast  Hot  Normal  Weak  Yes  Day14  Rain  Mild  High  Strong  No    42 BreakPage Example 4  Goal: Given a new instance X=<a ,…, a >, classify as Yes/No 1 n P(H ) x P(E|H )  n H argmax  i i  argmax  P(H ) x P(E|H ) argmax  P(H ) x P( a , a , a ,..., a |H )  argmax  P(H ) x P(a |H )  NB P(E) i i i 1 2 3 n i i j i Hi Hi Hi Hi j1  Naïve Bayes: Assumes that the attributes/features are  conditionally independent 43 BreakPage Example 4  Goal: Given a new instance X=<a ,…, a >, classify as Yes/No 1 n n H  argmax  P(H ) x P(a |H )  NB i j i Hi j1 1st estimate the probabilities from the training examples: For each hypothesis H a) i estimate P(H ) i For each attribute value a of each instance (evidence) b) j   estimate P(a | H ) i j 44 BreakPage Example 4 TRAIN: 1.  compute the probabilities from the training set P(PlayTennis  yes) 9 / 14 0.64 prior probabilities P(H ) P(PlayTennis no) 5 / 14 0.36 i P(Out sunny | PlayTennis  yes) 2 / 9 0.22 P(Out sunny | PlayTennis no) 3 / 5 0.60 P(Out rain | PlayTennis  yes) 3 / 9 0.33 conditional probabilities P(Out rain | PlayTennis no) 2 / 5 0.4 P(a | H ) i ... j P(Wind strong | PlayTennis  yes) 3 / 9 0.33 P(Wind strong | PlayTennis no) 3/ 5 0.60 45 BreakPage Example 4 TEST: 2. classify the new case:  X=(Outlook: Sunny, Temp: Cool, Hum: High, Wind: Strong) H  argmax  P(H ) x P(X | H ) NB i i H [yes,no] i       argmax   P(H ) x P(a | H ) i j i H [yes,no] j i       argmax  P(H ) x P(Outlook sunny | H ) x P(Temp cool | H ) i i i H [yes,no] i                   x P(Humidity high | H ) x P(Wind strong | H ) i i 1) P(PlayTennis yes)    x P(Outlook sunny | PlayTennis yes)xP(Temp cool | PlayTennis yes)xP(Hum high | PlayTennis yes)xP(Wind strong | PlayTennis yes)    0.0053 2) P(PlayTennis no)    x P(Outlook sunny | PlayTennis no)xP(Temp cool | PlayTennis no)xP(Hum high | PlayTennis no)xP(Wind strong | PlayTennis no)    0.0206  answer : PlayTennis (X) no 46 BreakPage Application of Bayesian Reasoning  Categorization: P(Category | Features of Object)  Diagnostic systems: P(Disease | Symptoms)  Text classification: P(sports_news | text)   Character recognition: P(character | bitmap)   Speech recognition: P(words | acoustic signal)  Image processing: P(face_person | image features)  Spam filter: P(spam_message | words in e-mail)  …  47 BreakPage Naive Bayes Classifier  A simple probabilistic classifier based on Bayes' theorem   with strong (naive) independence assumption  i.e. the features/attributes are conditionally independent  The assumption of conditional independence, often does  not hold…   But Naïve Bayes works very well in many applications  anyways!     ex: Medical Diagnosis  ex: Text Categorization (spam filtering) 48 BreakPage Ex. Application: Spam Filtering  Task: classify e-mails (documents) into a pre- defined class  ex: spam / ham  ex: sports, recreation, politics, war, economy,…  ex: customer email → order, complaint, support request, ...  Given  N sets of training texts (1 set for each class)  Each set is already tagged by the class name  Strictly speaking, what we will see is called a Multinomial Naïve Bayes  classifier, because we will count the number of words, as opposed to just  using binary values for the presence/absence of words…  49 BreakPage e-mail Representation  each e-mail is represented by a vector of feature/value:  feature = actual words in the e-mail  value = number of times that word appears in the e-mail  each e-mail in the training set is tagged with the correct  category. data  features / evidence / X  f(X)  instance  offer  money  viagra  laptop  exam  study  category  email 1  3  2  5  1  0  1  SPAM  email 2  1  1  0  5  4  3  HAM  email 3  0  3  2  1  0  1  SPAM  …                   task: correctly tag a new e-mail   offer  money  viagra  laptop  exam  study  category  new email  2  1  0  1  1  2  ?     50 BreakPage Naïve Bayes Algorithm // 1. training for all classes c // ex. ham or spam i    for all words w  in the vocabulary j count(w , c )  compute P(w | c )   j i j i  count(w , c )  j i j for all classes c icount(documents in c )  compute P(c )   i i count(all documents)  // 2. testing a new document D for all classes c  // ex. ham or spam i   score(c ) = P(c ) i i for all words w  in the D j score(c ) = score(c ) x P(w  | c ) i i j i choose c* = with the greatest score(c ) i   w   w   w   w   w   w   1 2 3 4 5 6 c1 : SPAM  p(w |c )  p(w |c )  p(w |c )  p(w |c )  p(w |c )  p(w |c )  1 1 2 1 3 1 4 1 5 1 6 1 c2 : HAM  p(w |c )  p(w |c )  p(w |c )  p(w |c )  p(w |c )  p(w |c )  1 2 2 2 3 2 4 2 5 2 6 2   51   BreakPage Example  Dataset  c1: SPAM doc1:  "cheap meds for sale" Spam doc2:  "click here for the best meds" doc3:  "book your trip"  c2: HAM doc4:   "cheap book sale, not meds" Ham doc5:   "here is the book for you"  Question:   doc6:  “the cheap book”  should it be classified as HAM or SPAM? ? → Worksheet #3 (“Email Spam Detection”) 52 BreakPage Be Careful: Smooth Probabilities (frequency of w  in c ) i j P(w | c )    normally: i j total number of words in c j  what if we have a P(w |c ) = 0…? i j  ex. the word "dumbo" never appeared in the class SPAM?   then P("dumbo"| SPAM) = 0  so if a text contains the word "dumbo", the class SPAM is  completely ruled out !  to solve this: we assume that every word always appears at  least once (or a smaller value)   ex: add-1 smoothing: (frequency of w  in c )  1 P(w | c )   i j i j total number  of words in c  size of vocabulary j 54 BreakPage Be Careful: Use Logs  if we really do the product of probabilities…   P  argmax  P(c )   P(w |c )  cj j i j  we soon have numerical underflow…  ex: 0.01 x 0.02 x 0.05 x …  so instead, we add the log of the probs Σ  argmax  log(P(c )) +   log(P(w |c)) cj j i  ex: log(0.01) + log(0.02) + log(0.05) + … 55 BreakPage Example COOKiNG SPORTS  Dataset c1: COOKING c2: SPORTS doc :   … stove… kitchen… the… heat doc :   … ball… heat… 1 1 doc :   … kitchen… pasta… stove… doc :   … the… referee… player… 2 2 … … doc :  … stove…heat… ball… doc :   goal… injury … 100000 75000  Assume:  |V| = 100     vocabulary = {ball, heat, kitchen, referee, stove, the, ... }  500,000 words in Cooking  300,000 words in Sports  100,000 docs in Cooking  75,000 docs in Sports 56 BreakPage Example  Training – Unsmoothed  / Smoothed  probs:  P(ball|COOKING) =          P(ball|SPORTS) =        P(heat|COOKING) =      P(heat|SPORTS) =        P(kitchen|COOKING) =      P(kitchen|SPORTS) =       P(referee|COOKING) =      P(referee|SPORTS) =      P(stove|COOKING) =      P(stove|SPORTS) =       P(the|COOKING) =      P(the|SPORTS) =       …  P(COOKING) =  P(SPORTS) =   Testing: “the referee hit the blue bird”  Score(COOKING)= log() + log(P(the|COOKING)) + log(P(referee|COOKING)) +                                                          log(P(hit|COOKING)) + log(P(the|COOKING))   Score(SPORTS)= log() + log(P(the|SPORTS)) + log(P(referee|SPORTS)) +                                                       log(P(hit|SPORTS)) + log(P(the| SPORTS))  57 BreakPage Example    58 BreakPage Another Application: Postal Code Recognition 59 BreakPage Digit Recognition  MNIST dataset  data set contains handwritten  digits from the American Census  Bureau employees and American  high school students  28 x 28 grayscale images  training set: 60,000 examples   test set: 10,000 examples.  Features: each pixel is used as a  feature so:  there are 28x28 = 784 features  each feature = 256 greyscale value  Task: classify new digits into one  of the 10 classes https://en.wikipedia.org/wiki/MNIST_database 60 BreakPage Image Classification 61 BreakPage Comments on Naïve Bayes Classification  Makes a strong assumption of conditional independence  that is often incorrect  ex: the word ambulance is not conditionally independent of the  word accident given the class SPORTS  BUT:   surprisingly very effec  tive on real-world tasks  basis of many spam filters  fast, simple  gives confidence in its class predictions (i.e., the scores)    Fast, easy to apply  often used as a baseline algorithm before trying other methods 62 BreakPage Today Introduction to ML 1. Naïve Bayes Classifier 2. Evaluation 3. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 63 BreakPage Evaluation of Learning Model  How do you know if what you learned is correct?  You run your classifier on a data set of unseen examples (that  you did not use for training) for which you know the correct  classification  Split data set into 3 sub-sets Actual training set (~80%) 1. ~80% Validation set (~20%) 2. Test set 3. ~20% 64 BreakPage Standard Methodology 1. Collect a large set of examples (all with correct classifications) 2. Divide collection into training, validation and test set Loop: 3. Apply learning algorithm to training set to learn the parameters 4. Measure performance with the validation set, and adjust hyper- parameters* to improve performance 5. Measure performance with the test set  DO NOT LOOK AT THE TEST SET  until step 5. Parameters:   Hyper-parameters:  parameters  basic values learned by the ML  used to set up the ML model. eg.  model. eg.  • for NB: value of delta for  • for NB: prior & conditional  smoothing,  probabilities  • for DTs: pruning level • for DTs: features to split • for ANNs: nb of hidden layers, nb  • for ANNs: weights  of nodes per layer… 65 BreakPage Metrics  Accuracy   % of instances of the test set the algorithm correctly  classifies  when all classes are equally important and represented   Recall, Precision & F-measure  when one class is more important than the others 66 BreakPage Accuracy  % of instances of the test set the algorithm correctly  classifies  when all classes are equally important and represented   problem:   when one class C is more important than the others  eg. when data set is unbalanced   Target  system 1    X1   X1     X2   X2     X3   X3     X4   X4     X5   X5     X6   X6     X7   X7     …  …    …  …    X500   X500   Accuracy    450/500 =  Accuracy                        = 495/500  90% !                                         = 99%                             67 BreakPage Recall, Precision  Recall: What proportion of the instances in class C are labelled  correctly?   Precision: What proportion of instances labeled with the class  C are actually correct?      In reality, the instance is…    in class C  Is not in class C    Model says…    instance is in class C  A  B     instance is NOT in class C  C  D      nb of instances that  are in  class C and that  nb of instances that are in   the model identified as  class C and that the model  A class C A identified as class C Precision = Recall= A+B nb of instances that the  A+C All instances that  model labelled as class C are in class C  68 BreakPage Example   Target  system 1  system 2  system 3    X1   X1   X1   X1     X2   X2   X2   X2     X3   X3   X3   X3     X4   X4   X4   X4     X5   X5   X5   X5     X6   X6   X6   X6     X7   X7   X7   X7     …   …  …   …     …   …  …   …     X500   X500   X500   X500   Accuracy    450/500 = 90% !  498/500 = 99.6%  498/500 = 99.6%        → Worksheet #3 (“Machine Learning System Evaluation”) 69 BreakPage Error Analysis  Where did the learner go wrong ?  Use a confusion matrix / contingency table   correct class  classes assigned by the learner  (that should have  been assigned)    C1  C2  C3  C4  C5  C6  …  Total  C1  94  3  0  0  3  0    100  C2  0  93  3  4  0  0    100  C3  0  1  94  2  1  2    100  C4  0  1  3  94  2  0    100  C5  0  0  3  2  92  3    100  C6  0  0  5  0  10  85    100  …                      71 BreakPage Today Introduction to ML 1. Naïve Bayes Classifier 2. Evaluation 3. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 72 BreakPage 