Artificial Intelligence:  Introduction to Neural Networks Perceptron, Backpropagation 1 BreakPage Today  Neural Networks   Perceptrons  Backpropagation https://www.linkedin.com/pulse/goedels-incompleteness-theorem-emergence-ai-eberhard-schoeneburg/ 2 BreakPage Neural Networks  Radically different approach to reasoning and  learning  Inspired by biology  the neurons in the human brain  Set of many simple processing units (neurons)  connected together  Behavior of each neuron is very simple  but a collection of neurons can have sophisticated  behavior and can be used for complex tasks   In a neural network, the behavior depends on  weights on the connection between the neurons  The weights will be learned given training data 3 BreakPage Biological Neurons  Human brain =   100 billion neurons  each neuron may be connected to  10,000 other neurons  passing signals to each other via  1,000 trillion synapses   A neuron is made of:  Dendrites: filaments that  provide input to the neuron  Axon: sends an output signal  Synapses: connection with other  neurons – releases  neurotransmitters to other  neurons Source: http://www.human-memory.net/brain_neurons.html 4 BreakPage Behavior of a Neuron  A neuron receives inputs from its neighbors  If enough inputs are received at the same time:  the neuron is activated   and fires an output to its neighbors  Repeated firings across a synapse increases its  sensitivity and the future likelihood of its firing  If a particular stimulus repeatedly causes activity in  a group of neurons, they become strongly associated 5 BreakPage Today  Neural Networks   Perceptrons  Backpropagation https://www.linkedin.com/pulse/goedels-incompleteness-theorem-emergence-ai-eberhard-schoeneburg/ 6 BreakPage A Perceptron  A single computational neuron        (no network yet…)  Input:   input signals x i   weights w  for each feature x i i   represents the strength of the connection with the neighboring  neurons  Output:  if sum of input weights >= some threshold, neuron fires (output=1)  otherwise output = 0  If (w x + … + w x ) >= t  1  1  n  n  Then output = 1  Else output = 0  Learning :  use the training data to adjust the weights in the percetron source: Luger (2005) 7 BreakPage The Idea Features (x ) Output i Student First  Male?  Works  Drinks First  last  hard?  ?  this  year?  year? Richard Yes  Yes No Yes  No Alan Yes  Yes Yes No  Yes  …   Step 1: Set weights to random values  1. Step 2: Feed perceptron with a set of inputs  2. Step 3: Compute the network outputs 3. Step 4: Adjust the weights  4. if output correct → weights stay the same  1. if output = 0 but it should be 1 →  2. increase weights on active connections (i.e. input x =1)  1. i  if output = 1 but should be 0 →  3. decrease weights on active connections (i.e. input x =1)  1. i  Step 5: Repeat steps 2 to 4 a large number of times until the network  5. converges to the right results for the given training examples source: Cawsey (1998) 8 BreakPage A Simple Example  Each feature (works hard, male, …) is an x i  if x  = 1, then student got an A last year, 1  if x  = 0, then student did not get an A last year, 1   …  Initially, set all weights to random values (all 0.2 here)  Assume:  threshold = 0.55   constant learning rate = 0.05 source: Cawsey (1998) 9 BreakPage A Simple Example (2) Features (x ) Output i Student ‘A’ last year?  Male?  Works hard?  Drinks?  ‘A’ this year? Richard Yes  Yes No Yes  No Alan Yes  Yes Yes No  Yes  Alison No  No Yes  No  No  Jeff No  Yes No  Yes  No Gail Yes  No Yes  Yes  Yes  Simon No Yes Yes  Yes  No   Richard:   → Worksheet #5 (“Perceptron”) 10 BreakPage A Simple Example (3) Features (x ) Output i Student ‘A’ last year?  Male?  Works hard?  Drinks?  ‘A’ this year? Richard Yes  Yes No Yes  No Alan Yes  Yes Yes No  Yes  Alison No  No Yes  No  No  Jeff No  Yes No  Yes  No Gail Yes  No Yes  Yes  Yes  Simon No Yes Yes  Yes  No   Alan:  → Worksheet #5 (“Perceptron Learning”)  After 2 iterations over the training set (2 epochs), we get:  w = 0.25 w = 0.1 w = 0.2 w = 0.1 1 2 3 4 12 BreakPage A Simple Example (3) Features (x ) Output i Student ‘A’ last year?  Male?  Works hard?  Drinks?  ‘A’ this year? Richard Yes  Yes No Yes  No Alan Yes  Yes Yes No  Yes  Alison No  No Yes  No  No  Jeff No  Yes No  Yes  No Gail Yes  No Yes  Yes  Yes  Simon No Yes Yes  Yes  No   Let’s check… (w = 0.2 w = 0.1 w = 0.25 w = 0.1) 1 2 3 4  Richard: (1✕0.2) + (1✕0.1) + (0✕0.25) + (1✕0.1) = 0.4 < 0.55 -> output is 0 ✓  Alan:      (1✕0.2) + (1✕0.1) + (1✕0.25) + (0✕0.1) = 0.55 ≥ 0.55 -> output is 1 ✓  Alison:   (0✕0.2) + (0✕0.1) + (1✕0.25) + (0✕0.1) = 0.25 < 0.55 -> output is 0 ✓  Jeff:     (0✕0.2) + (1✕0.1) + (0✕0.25) + (1✕0.1) = 0.2 < 0.55 -> output is 0 ✓  Gail:       (1✕0.2) + (0✕0.1) + (1✕0.25) + (1✕0.1) = 0.55 ≥ 0.55 -> output is 1 ✓  Simon:   (0✕0.2) + (1✕0.1) + (1✕0.25) + (1✕0.1) = 0.45 < 0.55 -> output is 0 ✓ 13 BreakPage Decision Boundaries of Perceptrons  So we have just learned the function:  If (0.2x + 0.1x  + 0.25x  + 0.1x  ≥ 0.55) then 1 otherwise 0 1  2 3 4  If (0.2x + 0.1x  + 0.25x  + 0.1x - 0.55 ≥ 0) then 1 otherwise 0 1  2 3 4   Assume we only had 2 features:  If (w x + w x  -t >= 0) then 1 otherwise 0  1 1  2 2  The learned function describes a line in the input space   This line is used to separate the two classes C1 and C2  t (the threshold, later called ‘b’) is used to shift the line on the axis  x 2 w x  + w x  –t ≥ 0 1 1 2 2 decision decision w x  + w x  -t = 0 C 1 1 2 2 boundary region for C1 1 x decision C 1 2 region for C 2 w x  + w x  -t < 0 1 1 2 2 14 BreakPage Decision Boundaries of Perceptrons  More generally, with n features, the learned function  describes a hyperplane in the input space.  x 2 decision region for C1 decision boundary decision x 1 region for C 2 x 3 15 BreakPage Adding a Bias  We can avoid having to   b  x w figure out the threshold  i i by using a “bias” i  A bias is equivalent to a  weight on an extra input  feature that always has  a value of 1. b w w 1 2 1 x x 1 2 16 BreakPage Perceptron - More Generally x  = 1 inputs bias input set to 1  0 x w (to replace the threshold)  1 1 w 0 output w 2 n x  O w x i  i 2 . i0 .  n   n   O  f w x  . w  f w x  i  i n i  i   transfer function   i0 x i0 n activation function final classification 17 BreakPage Common Activation Functions O O n n   w x w x i  i i  i i1 i0  Hard Limit activation functions:  n  1   if   w  x  t i i   step  O=   i1 0 otherwise  n   1   if   w  x  0 i i   sign O=  i0  -1 otherwise [Russell & Norvig, 1995] 18 BreakPage Learning Rate Learning rate can be a constant value (as in the previous example) 1. w  (T - O)  Error = target output – actual output learning rate  So:  if T=zero and O=1 (i.e. a false positive) -> decrease w by η  if T=1 and O=zero (i.e. a false negative) -> increase w by η  if T=O (i.e. no error) -> don’t change w Or, a fraction of the input feature x   2. i value of input feature x     i w   (T - O) x i i  So the update is proportional to the value of x  if T=zero and O=1 (i.e. a false positive) -> decrease w  by ηx i i  if T=1 and O=zero (i.e. a false negative) -> increase w  by ηx i i  if T=O (i.e. no error) -> don’t change w i  This is called the delta rule or perceptron learning rule 19 BreakPage Perceptron Convergence Theorem  Cycle through the set of training examples.  Suppose a solution with zero error exists.  The delta rule will find a solution in finite time. 20 BreakPage Example of the Delta Rule   plot of the training data:  training data:  perceptron source: Luger (2005) 21 BreakPage Let's Train the Perceptron  assume random initialization  w1 = 0.75  w2 = 0.5  w3 = -0.6  Assume:  sign function (threshold = 0)   learning rate η = 0.2 source: Luger (2005) 22 BreakPage Training  data #1:   data #2:   data #3:   ...  → Worksheet #5 (“Delta Rule”)  repeat… over 500 iterations, we converge to: w  = -1.3  w  = -1.1 w  = 10.9 1 2 3 source: Luger (2005) 23 BreakPage Remember this slide? 25 BreakPage Limits of the Perceptron  In 1969, Minsky and Papert showed  formally what functions could and  could not be represented by  perceptrons   Only linearly separable functions  can be represented by a  perceptron source: Luger (2005) 26 BreakPage AND and OR Perceptrons source: Luger (2005) 27 BreakPage A Perceptron Network  So far, we looked at  C1 a single perceptron ~C1 C2 ~C2  But if the output  needs to learn more  C1 ~C1 than a binary  (yes/no) decision C6 ~C6  Ex: learning to  recognize digit --> 10  possible outputs -->  need a perceptron  network 28 BreakPage Example: the XOR Function  We cannot build a perceptron to learn the exclusive-or function x1 x2 Output  To learn the XOR function, with: 1 1 0  two inputs x  and x   1 2  two weights w  and w 1 0 1 1 2  A threshold t 0 1 1 0 0 0  i.e. must have:  (1 ✕ w ) + (1 ✕ w ) < t (for the first line of truth table) 1 2  (1 ✕ w ) + 0 >= t 1  0 + (1 ✕ w )  >= t 2  0 + 0 < t   Which has no solution… so a perceptron cannot learn the XOR  function 29 BreakPage The XOR Function - Visually  In a 2-dimentional space (2 features for the X)  No straight line in two-dimensions can separate   (0, 1) and (1, 0) from   (0, 0) and (1, 1). source: Luger (2005) 30 BreakPage Non-Linearly Separable Functions   Real-world problems cannot always be represented by linearly- separable functions…   This caused a decrease in interest in neural networks in the 1970’s http://sebastianraschka.com/Articles/2014_kernel_pca.html 31 BreakPage Multilayer Neural Networks Non-linear, differentiable function  Solution:  to learn more complex  1. functions (more complex  decision boundaries), have  hidden nodes and for non-binary decisions,  2. have multiple output nodes use a non-linear activation  3. function http://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html 36 BreakPage The Sigmoid Function  Backpropagation requires a differentiable activation function  sigmoidal (or squashed or logistic) function                                                                                                       f returns a value between 0 and 1 (instead of  0 or 1)   f indicates how close/how far the output of the network is  compared to the right answer (the error term) 42 BreakPage Typical Activation Functions https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f 43 BreakPage Learning in a Neural Network  Learning is the same as in a perceptron:  feed network with training data  if there is an error (a difference between the output and  the target), adjust the weights  So we must assess the blame for an error to the  contributing weights 44 BreakPage Feed-forward + Backpropagation  Feed-forward:    Input from the features is  fed forward in the network  from input layer towards the  output layer           45 BreakPage Backpropagation  In a multilayer network…   Computing the error in the output layer is clear.  Computing the error in the hidden layer is not clear,  because we don’t know what its output should be  Intuitively:   A hidden node h is “responsible” for some fraction of the  error in each of the output node to which it connects.  So the error values (δ):  are divided according to the weight of their connection  between the hidden node and the output node   and are propagated back to provide the error values (δ) for  the hidden layer. 46 BreakPage Gradients  Gradient is just derivative in 1D    E(w) (w - 5) 2 E 2 w  5 Ex:                            derivative is:  w  If  w=3   E 3 2(3  5)  4 w E(w) negative slope/  derivative says increase w derivative (go in opposite direction  positive slope/  -> increase  of derivative) derivative weight -> decrease  weight  E(8) 2(8  5) 6 If w=8 w derivative says decrease w do nothing  (go in opposite direction  w w=3 w=5 w=8 of derivative) BreakPage Gradient Descent Visually (w ,w ) 1 2   (w +Δw ,w +Δw ) 1 1 2  2  Goal: minimize E(w1,w2) by changing w1 and w2  But what is the best combination of change in w1 and w2 to minimize E faster?  The delta rule is a gradient descent technique for updating the weights in a  single-layer perceptron. 48 BreakPage Gradient Descent Visually   Partial derivative  (or gradient) of E  with respect to w1 w 2 w 1 E  need to know how much a change in w1 will affect E(w1,w2) i.e w1 E  need to know how much a change in w2 will affect E(w1,w2) i.e  w2  Gradient ▽E points in the opposite direction of steepest decrease of E(w1,w2)  i.e. hill-climbing approach… Source: Andrew Ng BreakPage Training the Network After some calculus (see: https://en.wikipedia.org/wiki/Backpropagation) we get…  Note: To be consistent   Step 0: Initialise the weights of the network randomly  with Wikipedia, we’ll use  O-T instead of T-O, but  // feedforward we will subtract the   Step 1: Do a forward pass through the network (use sigmoid)  error in the weight  update     1  O  g  w x  sigmoid   w x    i  ji  j   ji  j    j   j     w ji x j  1  e  j  Derivative of sigmoid  // propagate the errors backwards note, if  we use g sigmoid :   Step 2:  For each output unit k, calculate its error term δ   k g' (x)  g(x) (1- g(x)) δ  g' (x )  Err  O (1 - O )  (O  - T ) k k k k k k k  Step 3: For each hidden unit h, calculate its error term δ   h Sum of the weighted error  δ  g' (x )  Err  O (1 - O )    w δ term of the output nodes  h h h h h hk k koutputs that h is connected to (ie.   Step 4: Update each network weight w : h contributed to the  ij errors δ ) w  w   Δw   where Δw   - η δ  O k ij ij ij ij j i  Repeat steps 1 to 4 until the error is minimised to a given level 50 BreakPage Example: XOR O 5  2 input nodes + 2 hidden nodes + 1 output node + 3 biases source: Negnevitsky, Artificial Intelligence, p. 181 51 BreakPage Example: Step 0  (initialization)  Step 0: Initialize the network at random θ  = 0.8 3 w  = 0.5 13 w  = -1.2 35 θ  = 0.3 w  = 0.4 5 23 O 5 w  = 0.9 14 w  = 1.1 45 w  = 1.0 24 θ  = -0.1 4 52 BreakPage Step 1: Feed Forward  Step 1: Feed the inputs and calculate the output    x x Target output T 1  O  sigmoid    w x    1 2 i  ji  j     j    wji xj  1 1 0 1  e  j        0 0 0 1 0 1 0 1 1 → Worksheet #5 (“Neural Network for XOR”) 53 BreakPage Step 2: Calculate error term of  output layer δ  g' (x )  Err  O (1 - O )  (O  - T ) k k k k k k k  Error term of neuron 5 in the output layer: → Worksheet #5 (“Neural Network for XOR”) O 5 δ  = error… will be  5 used to modify w   35 and w 45 56 BreakPage Step 3: Calculate error term of  hidden layer δ  g'(x )  Err  O (1 - O )    w δ h h h k k kh k koutputs  Error term of neurons 3 & 4 in the hidden layer:  δ  = O  (1-O )  δ w   3 3 3 5  35          = ... δ to modify   δ  = O  (1-O ) δ w   3  4 4 4 5  45 w  and w           = ... 13 23 O 5 → Worksheet #5  (“Neural Network for XOR”) δ  to modify  4 w  and w   14 24 58 BreakPage Step 4: Update Weights w  w   Δw   where Δw   -  δ  x ij ij ij ij j i  Update all weights (assume a constant learning rate η = 0.1)  Δw  = -η δ  x  = -0.1 x -0.0381 x 1 = 0.0038 13 3 1  Δw  = -η δ  x  =  14 4 1  Δw  = -η δ  x  = -0.1 x -0.0381 x 1 = 0.0038 23 3 2  Δw  = -η δ  x  =  24 4 2  Δw  = -η δ  O  = -0.1 x 0.1274 x 0.5250 = -0.00669 // O is seen as x (output of 3 is input to 5) 35 5 3 3  5   Δw  = -η δ  O  =   45 5 4  Δθ   = -η δ  (-1) = -0.1 x -0.0381 x -1 = -0.0038 3 3  Δθ   = -η δ  (-1) = -0.1 x 0.0147 x -1 = 0.0015 4 4 δ =-0.0381   Δθ   = -η δ  (-1) =  3 5 5 x =1  O 1 → Worksheet #5  5 (“Neural Network for XOR”) x =1  2 δ =0.0147 4 60 BreakPage Step 4: Update Weights (con't) w  w   Δw   where Δw   -  δ  x ij ij ij ij j i  Update all weights (assume a constant learning rate η = 0.1)  w  = w  + Δw  = 0.5 + 0.0038 = 0.5038 13 13 13  w  = w  + Δw  =  14 14 14  w  = w  + Δw  = 0.4 + 0.0038 = 0.4038 23 23 23  w  = w  + Δw  =  24 24 24  w  = w  + Δw  = -1.2 – 0.00669 = -1.20669 35 35 35  w  = w  + Δw  =  O 45 45 45 5  θ  = θ  + Δθ   = 0.8 - 0.0038 = 0.7962 3 3 3  θ  = θ  + Δθ   = -0.1 + 0.0015 = -0.0985 4 4 4  θ  = θ  + Δθ   =  5 5 5 → Worksheet #5 (“Neural Network for XOR” contd.) 62 BreakPage Step 4: Iterate through data        63 BreakPage The Result…  After 224 epochs, we get:  (1 epoch = going through all data once) θ  = -7.31 3 W  = 4.76 13 W  = -10.38 35 θ  = -4.56 W  = 4.76 5 23 O 5 W  = 6.39 14 W  = 9.77 45 W  = 6.39 24 θ  = -2.84 4 64 BreakPage Error is minimized Inputs Target Output Actual Output Error x x T O T-O 1 2 1 1 0 0.0155 -0.0155 0 1 1 0.9849 0.0151 1 0 1 0.9849 0.0151 0 0 0 0.0175 -0.0175   May be a local minimum… 65 BreakPage Stochastic Gradient Descent  Batch Gradient Descent (GD)   updates the weights after 1 epoch  can be costly (time & memory) since we need to evaluate the whole  training dataset before we take one step towards the minimum.  Stochastic Gradient Descent (SGD)   updates the weights after each training example  often converges faster compared to GD   but the error function is not as well minimized as in the case of GD  to obtain better results, shuffle the training set for every epoch  MiniBatch Gradient Descent:   compromise between GD and SGD  cut your dataset into sections, and update the weights after training on  each section 68 BreakPage Applications of Neural Networks  Handwritten digit recognition  Training set = set of handwritten digits (0…9)  Task: given a bitmap, determine what digit it represents  Input: 1 feature for each pixel of the bitmap    Output: 1 output unit for each possible character (only 1 should  be activated)  After training, network should work for fonts (handwriting)  never encountered  Related pattern recognition applications:   recognize postal codes  recognize signatures  … 74 BreakPage Applications of Neural Networks  Speech synthesis  Learning to pronounce English words  Difficult task for a rule-based system because English  pronunciation is highly irregular   Examples:  letter “c” can be pronounced [k] (cat) or [s] (cents)  Woman vs Women   NETtalk:  uses the context and the letters around a letter to learn how to  pronounce a letter  Input: letter and its surrounding letters  Output: phoneme 75 BreakPage NETtalk Architecture Ex:  a cat  → c is pronounced K  Network is made of 3 layers of units  input unit corresponds to a 7 character window in the text  each position in the window is represented by 29 input units  (26 letters + 3 for punctuation and spaces)  26 output units – one for each possible phoneme  Listen to the output through iterations:  https://www.youtube.com/watch?v=gakJlr3GecE source: Luger (2005) 76 BreakPage Neural Networks  Disadvantage:   result is not easy to understand by humans (set of  weights compared to decision tree)… it is a black box  Advantage:   robust to noise in the input (small changes in input do not  normally cause a change in output) and graceful  degradation 77 BreakPage Today  Introduction to Neural Networks   Perceptrons  Backpropagation 79 BreakPage 