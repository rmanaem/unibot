Artificial Intelligence:  ML: Decision Trees  & k-means Clustering    Russell & Norvig: Sections 18.3, 18.4 1 BreakPage Today Introduction to ML (contd.) 1. Decision Trees 2. Evaluation (contd.) 3. Unsupervised Learning: k-means Clustering  4. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 2 BreakPage Applications  Too many to list here!  Recommender systems (eg. Netflix)   Pattern Recognition (eg. Handwriting recognition)  Detecting credit card fraud  Computer vision (eg. Object recognition)  Discovering Genetic Causes of Diseases   Natural Language Processing (eg. Spam filtering)  Speech Recognition / Synthesis  Medical Diagnostics  Information Retrieval (eg. Image search)  Learning heuristics for game playing  …  Oh… I’m out of space   BreakPage What is Machine Learning?  Learning = crucial characteristic of an intelligent  agent  ML  Constructs algorithms that learn from data  i.e., perform tasks that were not explicitly programmed  and improve their performance the more tasks they  accomplish  generalize from given experiences and are able to make  judgments in new situations  5 BreakPage Types of Machine Learning http://www.cognub.com/index.php/cognitive-platform/ 6 BreakPage Types of Machine Learning  Supervised learning  We are given a training set of (X, f(X)) pairs  X = <color, length>  ? sea bass salmon   Unsupervised learning  We are only given the Xs - not the corresponding f(X) ? 7 BreakPage Types of Learning  In Supervised learning  We are given a training set of (X, f(X)) pairs  big nose big teeth big eyes no moustache f(X) = not person small nose small teeth small eyes no moustache f(X) = person small nose big teeth small eyes moustache f(X) = ?  In Reinforcement learning  We are not given the (X, f(X)) pairs small nose big teeth small eyes moustache f(X) = ?  But we get a reward when our learned f(X) is right, and we try to maximize the reward  Goal: maximize the nb of right answers    In Unsupervised learning  We are only given the Xs - not the corresponding f(X) big nose big teeth big eyes no moustache not given small nose small teeth small eyes no moustache not given small nose big teeth small eyes moustache f(X) = ?  No teacher involved / Goal: find regularities among the Xs (clustering)  Data mining 8 BreakPage Logical Inference  Inference: process of deriving new facts  from a set of premises  Types of logical inference: Deduction 1. Abduction  2. Induction 3. 9 BreakPage Deduction  aka Natural Deduction  Conclusion follows necessary from the premises.    From A  B and A, we conclude that B   We conclude from the general case to a specific  example of the general case   Ex: All men are mortal. Socrates is a man. Socrates is mortal. 10 BreakPage Abduction  Conclusion is one hypothetical (most probable)  explanation for the premises  From A ⇒ B and B, we conclude A  Ex: Drunk people do not walk straight. John does not walk straight. John is drunk.  Not sound… but may be most likely explanation for B  Used in medicine… ⇒  in reality… disease   symptoms  patient complains about some symptoms… doctor concludes a disease  11 BreakPage Induction  Conclusion about all members of a class from the  examination of only a few member of the class.    From A∧C ⇒ B and A∧D ⇒ B, we conclude A⇒B  We construct a general explanation based on a specific  case.   Ex: All CS students in COMP 6721 are smart. All CS students on vacation are smart. All CS students are smart.  Not sound  But, can be seen as  hypothesis construction or generalisation 12 BreakPage Inductive Learning  = learning from examples   Most work in ML   Examples are given (positive and/or negative) to train a  system in a classification (or regression) task  Extrapolate from the training set to make accurate  predictions about future examples  Can be seen as learning a function  Given a new instance X you have never seen  You must find an estimate of the function f(X)  where f(X) is  the desired output  Ex:  small nose big teeth small eyes moustache f(X) = ? X  X = features of a face (ex. small nose, big teeth, …)  f(X) = function to tell if X represents a human face or not  13 BreakPage Types of Machine Learning http://www.cognub.com/index.php/cognitive-platform/ 14 BreakPage Example  Given  pairs (X,f(X)) (the training set – the data points)  Find a function that fits the training set well  So that given a new X, you can predict its f(X) value Possible decision  boundary b Other possible decision  a s boundary s r o l decision regions o c s a l m o n length  Note: choosing one function over another beyond just looking at the  training set is called inductive bias (eg. prefer "smoother" functions) 15 BreakPage Inductive Learning Framework  Input data are represented by a vector of features, X  Each vector X is a list of (attribute, value) pairs.  Ex: X = [nose:big, teeth:big, eyes:big, moustache:no]   The number of attributes is fixed (positive, finite)  Each attribute has a fixed, finite number of possible values   Each example can be interpreted as a point in a n-dimensional  feature space  where n is the number of attributes Note: attribute == feature 16 BreakPage Example Real ML applications typically require hundreds, thousands or millions of examples source: Alison Cawsey: The Essence of AI (1997). 17 BreakPage Techniques in ML  Probabilistic Methods  ex: Naïve Bayes Classifier  Decision Trees  Use only discriminating features as questions in a big if-then-else  tree  Neural networks   Also called parallel distributed processing or connectionist systems  Intelligence arise from having a large number of simple  computational units  … 18 BreakPage Today Introduction to ML (contd.) 1. Decision Trees 2. Evaluation (contd.) 3. Unsupervised Learning: k-means Clustering  4. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 19 BreakPage Guess Who? 20 BreakPage Decision Trees  Simplest, but most successful form of learning algorithm  Very well-know algorithm is ID3 (Quinlan, 1987) and its  successor C4.5  Look for features that are very good indicators of the  result, place these features (as questions) in nodes of  the tree  Split the examples so that those with different values  for the chosen feature are in a different set   Repeat the same process with another feature 21 BreakPage ID3 / C4.5 Algorithm  Top-down construction of the decision tree   Recursive selection of the “best feature” to use at the current  node in the tree  Once the feature is selected for  the current node, generate  children nodes, one for each  possible value of the selected  attribute  Partition the examples using the  possible values of this attribute,  and assign these subsets of the  examples to the appropriate child  node  Repeat for each child node until  all examples associated with a  node are classified http://www.rulequest.com/Personal/ BreakPage Example Info on last year’s students to determine if a student will get an ‘A’ this year Features (X) Output f(X) Student ‘A’ last  Black  Works  Drinks?  ‘A’ this year? year?  hair?  hard?  X1: Richard Yes  Yes No Yes  No  X2: Alan Yes  Yes  Yes No  Yes  X3: Alison No  No  Yes  No  No   X4: Jeff No  Yes  No  Yes  No X5: Gail Yes  No Yes  Yes  Yes  X6: Simon No  Yes  Yes  Yes  No  → Worksheet #4 (Decision Tree) 23 BreakPage Example 2: The Restaurant  Goal: learn whether one should wait for a table  Attributes  Alternate: another suitable restaurant nearby  Bar: comfortable bar for waiting  Fri/Sat: true on Fridays and Saturdays  Hungry: whether one is hungry  Patrons: how many people are present (none, some, full)  Price: price range ($, $$, $$$)  Raining: raining outside  Reservation: reservation made  Type: kind of restaurant (French, Italian, Thai, Burger)  WaitEstimate: estimated wait by host (0-10 mins, 10-30, 30-60, >60) 25 BreakPage Example 2: The Restaurant  Training data: source: Norvig (2003) 26 BreakPage A First Decision Tree  But is it the best decision tree we can build? source: Norvig (2003) 27 BreakPage Ockham’s Razor Principle    “It is vain to do more than can be done  with less… Entities  should not be  multiplied beyond necessity.“ [Ockham, 1324]  In other words… always favor the simplest answer  that correctly fits the training data  i.e. the smallest tree on average  This type of assumption is called inductive bias  inductive bias = making a choice beyond what the training  instances contain 28 BreakPage Finding the Best Tree empty tree  can be seen as searching the space  of all possible decision trees  Inductive bias: prefer shorter  trees on average  how?  search the space of all decision  trees   always pick the next attribute to  split the data based on its  "discriminating power"  (information gain)  in effect, steepest ascent hill- climbing search where heuristic is  information gain complete tree source: Tom Mitchell, Machine Learning (1997) 29 BreakPage Which Tree is Best?  F1? F2? F3? F4? F5? F6? F7? class class class class class class class class F1? F2? class F3? class F4? class class F5? class F6? class F7? class class 30 BreakPage Smaller trees are better What’s the size of a tree?  Number of leaves  Height of the tree  Longest path in the tree from the root to a leaf  External Path Length  Start at leaf, go up to the root and count the number of  edges   Do this for every leaf and add up the numbers  Weighted External Path Length  Idea: not all paths are equally important/likely  Use the training data to computed a weighted sum 31 BreakPage Choosing the Next Attribute  The key problem is choosing which  feature to split a given set of examples  ID3 uses Maximum Information-Gain:  Choose the attribute that has the largest  information gain  i.e., the attribute that will result in the smallest  expected size of the subtrees rooted at its  children    information theory 32 BreakPage Intuitively… Output f(X)  Patron:  If value is Some… all outputs=Yes  If value is None… all outputs=No  If value is Full… we need more tests  Type:  If value is French… we need more tests  If value is Italian… we need more tests  If value is Thai… we need more tests  If value is Burger… we need more tests  …  So patron  may lead to shorter tree… source: Norvig (2003) 33 BreakPage Next Feature…  For only data where patron = Full  hungry:  If value is Yes… we need more tests  If value is No… all output= No  type:  If value is French… all output= No  If value is Italian… all output= No  If value is Thai… we need more tests  If value is Burger… we need more tests  …  So hungry is more discriminating (only 1 new branch)… 34 BreakPage A Better Decision Tree  4 tests instead of 9   11 branches instead of 21  source: Norvig (2003) 35 BreakPage Essential Information Theory  Developed by Shannon in the 1940s  Notion of entropy (information content)  Measure how “predictable” a RV is...    If you already have a good idea about the answer (e.g. 90/10 split) → low entropy  If you have no idea about the answer (e.g. 50/50 split) → high entropy 36 BreakPage Choosing the Next Attribute  The key problem is choosing which feature to  split a given set of examples  Most used strategy: information theory    H(X)   p(x )log p(x ) Entropy (or information content) i 2 i x X i  1 1   H(fair coin  toss)   p(x )log p(x )  H ,  i 2 i  2 2  x X i  1 1 1 1     log  log  1 bit 2 2  2 2 2 2  entropy of a fair coin toss (the  RV) with 2 possible outcomes,  each with a probability of 1/2  37 BreakPage Why -p(x)·log (p(x)) 2 38 BreakPage Entropy  Let X be a discrete random variable (RV) with i possible  outcomes x i  Entropy (or information content) n  H(X)   p(x )log p(x ) i 2 i i1  measures the amount of information in a RV  average uncertainty of a RV  the average length of the message needed to transmit an  outcome x  of that variable  i  measured in bits  for only 2 outcomes x  and x , then 1 ≥ H(X) ≥ 0  1 2 → Worksheet #4 (Information Content) 39 BreakPage Example: The Coin Flip n  1 1 1 1   Fair coin: H(X)   p(x )log p(x )  -  log  log  1 bit i 2 i 2 2  2 2 2 2  i1 n  99 99 1 1   Rigged coin: H(X)   p(x )log p(x )  -  log  log  0.08 bits i 2 i 2 2  100 100 100 100  i1 fair coin -> high entropy y p o r t n E rigged coin -> low entropy P(head) → Worksheet #4 (Entropy) 40 BreakPage Choosing the Best Feature (con't)  The "discriminating power" of an attribute A given a data set S  Let Values(A)  = the set of values that attribute A can take  Let S  = the set of examples in the data set which have value v for  v attribute A (for each value v from Values(A) ) information gain  (or entropy reduction) gain(S,  A) H(S)  H(S | A) S                  H(S)   v x HS  v S v  values(A) 41 BreakPage Some Intuition Size Color Shape Output Big Red Circle + Small Red Circle + Small Red Square - Big Blue Circle -  Size is the least discriminating attribute (i.e.  smallest information gain)  Shape and color are the most discriminating  attributes (i.e. highest information gain) 42 BreakPage A Small Example (1) Size Color Shape Output Values(Color) = {red,blue} Big Red Circle + n Color    Small Red Circle + n Red: 2+ 1-   n Blue: 0+ 1-  Small Red Square - Big Blue Circle - S gain(S, Color)  H(S)   v x HS  v  2 2 2 2  S H(S)   log  log  1 v  values(Col or) 2 2  4 4 4 4  for each v of Values(Color)  2 1   2 2 1 1         H(S| Color  red) H ,    log  log  0.918 2 2  3 3   3 3 3 3   1 1           H(S| Color  blue) H 1,0   log  0 2  1 1  3 1 H(S| Color)  (0.918)  (0)  0.6885 4 4         gain(Color)  H(S) - H(S| Color)  1 - 0.6885 0.3115 43 BreakPage A Small Example (2)  Size Color Shape Output n Shape   Big Red Circle + n Circle: 2+ 1-n   S  quare: 0+ 1-           Small Red Circle + Small Red Square - Big Blue Circle - Note: by definition,  Log 0 = -∞   2 2 2 2  H(S)   log  log  1  0log0 is 0 2 2  4 4 4 4  3 1 H(S| Shape)  (0.918)  (0) 0.6885 4 4 gain(Shape)  H(S) - H(S| Shape)  1 - 0.6885 0.3115 44 BreakPage A Small Example (3) Size Color Shape Output Big Red Circle + n Size   Small Red Circle + n Big: 1+ 1- n Small: 1+ 1-    Small Red Square - Big Blue Circle -  2 2 2 2  H(S)   log  log  1 2 2  4 4 4 4  → Worksheet #4 (“Information Gain”) 46 BreakPage A Small Example (4) Size Color Shape Output Big Red Circle + Small Red Circle + Small Red Square - Big Blue Circle - gain(Shape )  0.3115 gain(Color )  0.3115 gain(Size)   0  So first separate according to either color or shape  (root of the tree) 47 BreakPage A Small Example (5)  Let’s assume we pick Color for the root… Color Size Color Shape Output blue red Big Red Circle + S Size? or  - Small Red Circle + 2 Shape? Small Red Square - Big Blue Circle -  2 2 1 1  H(S )   log  log  2 2 2  3 3 3 3  0 Size 48 BreakPage Back to the Restaurant  Training data: source: Norvig (2003) 50 BreakPage The Restaurant Example gain(alt) ... gain(bar) ... gain(fri) ... gain(hun) ...  2  0 2  4  0 4  6  2 4   gain(pat) 1    x H ,    x H ,    x H ,      12  2 2  12  4 4  12  6 6    2  0 0 2 2  4  0 0 4 4   1    x -   log   log   x    log     log   ...  0.541bits   2 2 2 2  12  2 2 2 2  12  4 4 4 4   gain(price ) ... gain(rain) ... gain(res) ...  2  1 1  2  1 1  4  2 2  4  2 2   gain(type) 1    x H ,    x H ,    x H ,   x H ,   0 bits    12  2 2  12  2 2  12  4 4  12  4 4   gain(est) ...  Attribute pat (Patron) has the highest gain, so root of the  tree should be attribute Patrons  do recursively for subtrees 51 BreakPage Decision Boundaries of  Decision Trees Feature 1 Feature 2 52 BreakPage Decision Boundaries of  Decision Trees Feature 1 Feature 2 > t1 ?? Feature 2 t1 53 BreakPage Decision Boundaries of  Decision Trees Feature 1 Feature 2 > t1 t2 Feature 1 > t2 Feature 2 t1 ?? 54 BreakPage Decision Boundaries of  Decision Trees Feature 2 > t1 Feature 1 Feature 1 > t2 t2 Feature 2 t3 t1 Feature 2 > t3 55 BreakPage Applications of Decision Trees  One of the most widely used learning methods in  practice   Fast, simple, and traceable   Can out-perform human experts in many problems 56 BreakPage Today Introduction to ML (contd.) 1. Decision Trees 2. Evaluation (contd.) 3. Unsupervised Learning: k-means Clustering  4. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 57 BreakPage Metrics, revisited  Accuracy   % of instances of the test set the algorithm correctly  classifies  when all classes are equally important and represented   Recall, Precision & F-measure  when one class is more important and the others 60 BreakPage Confusion Matrix  Not all errors are equal  Type I error (FP) might be worse than Type II error (FN) (depends on the application, e.g., spam filtering)  “It is better to risk saving a guilty man than to condemn an  innocent one.” (Voltaire)     In reality, the instance is…    in class C  Is not in class C    Model says…    instance is in class C  True Positive  False Positive   (TP)  (FP)     instance is NOT in class C  False Negative  True Negative  (FN)  (TN)      TTTPPP TP+TN TP Precision = RRReeecccaaallllll=== Accuracy=   TP+FP TTTPPP+++FFFNNN TP+TN+FP+FN 62 BreakPage Evaluation: A Single Value Measure  cannot take mean of P&R  if R = 50%    P = 50%  M = 50%  if R = 100%  P = 10%  M = 55% (not fair)  take harmonic mean 2 HM is high only when both P&R are high HM  1 1 if R = 50% and P = 50%    HM = 50%  R P if R = 100% and P = 10%   HM = 18.2%  take weighted harmonic mean w : weight of R  w : weight of P  a = 1/w   b= 1/w r p r p (a  b) a 1 a  b b b WHM          a  b a  b a  1 R P Rb Pb bR P  let β2 = a/b                                                       β2  1 (β2  1) PR WHM               β2  β2P  R   1  R P   … which is called the F-measure 64 BreakPage Evaluation: the F-measure  A weighted combination of precision and recall (β2  1)PR F  (β2P  R)  β represents the relative importance of precision  and recall  when β = 1, precision & recall have same importance  when β > 1, precision is favored  when β < 1, recall is favored 65 BreakPage Example ✔ ❌ ✔ ✔ ✔ ❌ ❌ ✔ ✔ ❌ ✔ ✔ ✔ ❌ ✔ ✔ ✔ ❌ ❌ ✔ ❌ ❌ ❌ ✔ ❌ ❌ ❌ ✔ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ (B=1)       → Worksheet #4 (F-Measure)   67 BreakPage Error Analysis  Where did the learner go wrong ?  Use a confusion matrix / contingency table   correct class  classes assigned by the learner  (that should have  been assigned)    C1  C2  C3  C4  C5  C6  …  Total  C1  94  3  0  0  3  0    100  C2  0  93  3  4  0  0    100  C3  0  1  94  2  1  2    100  C4  0  1  3  94  2  0    100  C5  0  0  3  2  92  3    100  C6  0  0  5  0  10  85    100  …                      68 BreakPage A Learning Curve   Size of training set   the more, the better  but after a while, not much improvement… source: Mitchell (1997) 69 BreakPage Some Words on Training  In all types of learning… watch out for:  Noisy input  Overfitting/underfitting the training data 70 BreakPage Noisy Input  In all types of learning… watch out for:  Noisy Input:  Two examples have the same feature-value pairs, but different  outputs  Size Color Shape Output Big Red Circle + Big Red Circle -  Some values of features are incorrect or missing (ex. errors in  the data acquisition)  Some relevant attributes are not taken into account in the data  set  71 BreakPage Overfitting  If a large number of irrelevant features are there, we may find  meaningless regularities in the data that are particular to the  training data but irrelevant to the problem.  Complicated boundaries  overfit the data  they are too tuned to the  particular training data at  hand  They do not generalize well to  the new data   Extreme case: “rote learning”  Training error is low  Testing error is high  BreakPage Underfitting  We can also underfit data, i.e.  use too simple decision  boundary   Model is not expressive  enough (not enough features)  There is no way to fit a linear  decision boundary so that the  training examples are well  separated  Training error is high  Testing error is high  BreakPage Cross-validation  K-fold cross-validation   run k experiments, each time you test on 1/k of the data, and train on the rest  than you average the results  ex: 10-fold cross validation 1. Collect a large set of examples (all with correct classifications) 2. Divide collection into two disjoint sets:  training (90%) and test (10% = 1/k) 3. Apply learning algorithm to training set  4. Measure performance with the test set 5. Repeat steps 2-4, with the 10 different portions 6. Average the results of the 10 experiments exp1: train test exp2: train test train exp3: train test train … … 74 BreakPage Today Introduction to ML (contd.) 1. Decision Trees 2. Evaluation (contd.) 3. Unsupervised Learning: k-means Clustering  4. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 75 BreakPage Types of Machine Learning http://www.cognub.com/index.php/cognitive-platform/ 76 BreakPage Remember this slide? sea bass 77 BreakPage Unsupervised Learning  Learn without labeled examples   i.e. X is given, but not f(X)  small nose big teeth small eyes moustache f(X) = ?  Without a f(X), you can't really identify/label a  test instance  But you can:  Cluster/group the features of the test data into a  number of groups   Discriminate between these groups without  actually labeling them 78 BreakPage What is Clustering  The organization of unlabeled data into similarity groups  called clusters.  A cluster is a collection of data items which are “similar”  between them, and “dissimilar” to data items in other  clusters. 79 BreakPage Historic Application of Clustering  John Snow, a London physician plotted the  location of cholera on a map during an  outbreak in the 1850s.  The locations indicated that cases were  clustered arounds certain intersections  where there were polluted wells – thus  exposing both the problem and the solution. FROM: Nina Mishra HP Labs 80 BreakPage Clustering  Represent each instance as a vector <a , a , a ,…, a > 1 2 3 n  Each vector can be visually represented in an n-dimensional  space a a a Output 1 2 3 X 1 0 0 ? 1 X X 1 6 0 ? 5 2 X 8 0 1 ? X 3 2 X 6 1 0 ? 4 X 1 7 1 ? 5 X 4 X 1 X 3 81 BreakPage Clustering  Clustering algorithm  Represent test instances on a n dimensional space   Partition them into regions of high density  How?  … many algorithms (ex. k-means)  Compute the centroïd of each region as the   average of data points in the cluster 82 BreakPage Clustering Techniques K-means 83 BreakPage k-means Clustering  User selects how many clusters they want… (the value of k) 1. Place k points into the space (ex. at random).  These points represent initial group centroïds. 2. Assign each data point x to the nearest centroïd. n 3. When all data points have been assigned,  recalculate the positions of the K centroïds as the  average of the cluster 4. Repeat Steps 2 and 3 until none of the data  instances change group. 84 BreakPage Euclidean Distance  To find the nearest  centroïd… 10  a possible metric is the  9 Euclidean distance 8  distance between 2 pts  7 p = (p , p , ....,p ) 1 2 n 6 q = (q , q , ....,q )   1 2 n n 5 d   p  q 2 i i 4 i1  where to assign a data point  3 x? 2  For all k clusters, chose the  1 one where x has the  smallest distance 1 2 3 4 5 6 7 8 9 10 85 BreakPage Example  (in 2-D… i.e. 2 features) initial 3 centroïds (ex. at random) 5 4 c 1 3 c 2 2 1 c 3 0 0 1 2 3 4 5 86 BreakPage Example partition data points to closest centroïd 5 4 c 1 3 c 2 2 1 c 3 0 0 1 2 3 4 5 87 BreakPage Example re-compute new centroïds 5 4 c 1 3 2 c 3 c 1 2 0 0 1 2 3 4 5 88 BreakPage Example re-assign data points to new closest centroïds 5 4 c 1 3 2 c 3 c 1 2 0 0 1 2 3 4 5 89 BreakPage Example 5 4 c 1 3 2 c 2 c 3 1 0 0 1 2 3 4 5 → Worksheet #4 (k-Means Clustering) 90 BreakPage Why use k-means?  Strengths:  Simple  Easy to understand and implement  Efficient: Time complexity O(t·k·n)  n number of data points  k number of clusters  t number of iterations  With small k and t, linear performance on practical  problems 92 BreakPage Weakness of k-means  User needs to specify k  Algorithm is sensitive to outliers  i.e., data points that are far away from others  Could be errors in the data or special data  points with very different characteristics 93 BreakPage Outliers outlier (A) Undesirable clusters outlier (B) Ideal clusters 94 BreakPage Special data structures (A) Two natural clusters                                      (B) k-means clusters 95 BreakPage Sensitivity to initial seeds 96 BreakPage K-means: Summary  Despite weaknesses, k-means is still one of  the most popular algorithms, due to its  simplicity and efficiency  No clear evidence that any other  clustering algorithm performs better in  general  Comparing different clustering algorithms  is a difficult task.  No one knows the correct clusters! 97 BreakPage Today Introduction to ML (contd.) 1. Decision Trees 2. Evaluation (contd.) 3. Unsupervised Learning: k-means Clustering  4. https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b 98 BreakPage 