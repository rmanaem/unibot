Artificial Intelligence:  Deep Learning many slides from:  Y. Bengio, A. Ng and Y. LeCun 1 BreakPage Today 1. Motivation 2. Feature Learning 3. Training a Deep Neural Network 4. CNNs for Image Processing 5. Conclusion 2 BreakPage History of AI 3 BreakPage Deep Learning in the Academic Press  (2012-2015) 4 BreakPage Deep Learning in the News  (2013) Slide from Yoshua Bengio, 2015 5 BreakPage Deep Learning in the News  (2012-2014) 6 Slide from Yoshua Bengio, 2015 BreakPage Major Breakthroughs   Speech Recognition & Machine Translation (2010+)  Skype Translator Skype Translator Google Translate Google now  Image Recognition & Computer Vision (2012+)  Natural Language Processing (2014+)   … 7 Figure from Yoshua Bengio, 2015 BreakPage Major Breakthroughs   Speech Recognition & Machine Translation (2010+)   Image Recognition & Computer Vision (2012+) Object recognition  Self driving cars 3.581% 3.567% 4th year  Natural Language Processing (2014+)  … 8 Figure from Yoshua Bengio, 2015 BreakPage Major Breakthroughs   Speech Recognition & Machine Translation (2010+)  Image Recognition & Computer Vision (2012+)  Natural Language Processing (2014+) Question Answering Image Captioning (deep vision + deep NLP) 9 BreakPage Image Captioning: Better than humans? A B 10 BreakPage Today 1. Motivation 2. Feature Learning 3. Training a Deep Neural Network 4. CNNs for Image Processing 5. Deep Learning for NLP 6. Conclusion 11 BreakPage A Deep Neural Net The Google “Inception” deep neural network architecture for image recognition  (27 layers)  https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c 12 https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/ BreakPage Initial Drawbacks 1. Standard backpropagation with sigmoid activation function  does not scale well with multiple layers  Weight of early layers change too slowly (no learning) 2. Overfitting  Large network -> lots of parameters -> increased capacity to  “learn by heart” 3. Multilayered ANNs need lots of labeled data   Most data is not labeled :( 13 BreakPage Initial Drawbacks (1) 1. Standard gradient-based backpropagation does not scale well with  multiple layers… When we multiply the gradients many times (for  each layer),  it can lead to … a) Vanishing gradient problem:   gradients shrink exponentially with the  number of layers   so weight updates get smaller and smaller  and weights of early layers change very  slowly and network learns very very slowly b) Exploding gradient problem:  multiplying gradients could also make them  grow exponentially.    so weight updates get larger and larger  and the weights can become so large as to  overflow and result in NaN values 14 BreakPage Initial Drawbacks (1) To help, we can : a) Use other activation  functions … b) Do “gradient clipping”  (i.e. set bounds on the  gradients) https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f 15 https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6 BreakPage Initial Drawbacks (2) 2. Overfitting  Large network -> lots of parameters ->  increased capacity to “learn by heart”  Solutions:  Regularization:   modify the error function that we minimize to penalize large weights.    where f(w) grows larger as the weights grow larger and λ is the  regularization strength  Dropout:   keep a neuron active with some probability p or setting it to  zero otherwise.   prevents the network from becoming too dependent on any  one neuron.  https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html 16 BreakPage Initial Drawbacks (3) 3. Multilayered ANNs need lots of labeled data   Solution: “pre-train” the network with features found  automatically using unsupervised data   i.e. Automatic feature learning… (see next few slides) 17 BreakPage Classic ML Learning algorithm Input Classic ML,  requires labeled  Manual Extraction of Features  data and hand- (eg. edge detection, colors,  crafted features texture,…) 2 1. Needs expert    e knowledge r u t a 2. Time-consuming  e F      and expensive Motorbikes “Non”-Motorbikes 3. Does not          generalize to      other domains Feature 1 Slide from Y. LeCun 18 BreakPage Automatic Feature Learning handle Automatic  Learning Feature  algorithm wheel Representation eg. handle, wheel, …  With Automatic  Input Feature Learning: 1. We feed the  network the raw data  (not feature- ” curated) e l d n Motorbikes a 2. The features are  h “ “Non”-Motorbikes learned by the  network 3. Features learned  can be re-used in  similar tasks.  “wheel” Slide from Y. LeCun 19 BreakPage Automatic Feature Learning https://www.strong.io/blog-images/movie-posters/Slide6.png 20 BreakPage Automatic Feature Learning Deep Learning = Machine learning algorithms based on  learning multiple levels of representation / abstraction.  – Y. Bengio  Each layer learns more abstract features that are then combined /  composed into higher-level features automatically  Like the human brain …   has many layers of neurons which act as feature detectors  detecting more and more abstract features as you go up  E.g. to classify an image of a cat:  Bottom Layers: Edge detectors, curves, corners straight lines  Middle Layers: Fur patterns, eyes, ears  Higher Layers: Body, head, legs  Top Layer: Cat or Dog 21 BreakPage Automatic Feature Learning 22 BreakPage What Types of Features?  For image recognition   pixel -> edge -> texton -> motif -> part -> object  For NLP  character -> word ->  constituents -> clause -> sentence -> discourse  For speech:  sample → spectral band -> sound -> … phone -> phoneme -> word 23 Figure from Y LeCun BreakPage Eg: Learning Image Features Examples of learned objects parts from object categories Chairs Elephants Faces Cars Learned  Learned features /  objects representations can be used in   variety of OTHER classification  tasks… → deep learning Learned  object  parts Learned  edges Actual images  (pixels) 24 BreakPage Advantages of Unsupervised Feature  Learning  25 BreakPage Advantages of Unsupervised  Feature Learning   Much more unlabeled data available than labeled data:  Eg. Websites, Books, Videos, Pictures  Humans learn initially from unlabeled examples  Eg. Babies learn to talk/recognize objects without labeled data  As the features are learned in an unsupervised way from a  different and larger dataset, less risk of over-fitting  No need for manual feature engineering   These features are organized into multiple levels  Each level creates new features from combinations of features from  the level below  Each level is more abstract than the ones below (hierarchy of  features) 26 BreakPage Today 1. Motivation 2. Feature Learning 3. Training a Deep Neural Network 4. CNNs for Image Processing 5. Deep Learning for NLP 6. Conclusion 27 BreakPage General Architecture of a Deep Network 1. Unsupervised pre-training of neural network using  unlabeled data  aka. unsupervised learning of features 2. Supervised training with labeled data using features  learned from above with a standard classifier  Eg. an ANN, SVM, …  28 BreakPage Training a Deep NN train this  then this  then this  then this  finally  layer first layer layer layer this layer EACH of the (non-output) layers is trained to  Use pretrained  learn a representation of the data (eg.  representations to feed a  autoencoder) aka “pretraining the network  regular ANN with a smaller  with unsupervised data” set of labelled data.  29 BreakPage Autoencoders  To learn a representation of the data, we can use: Deep Belief Networks  1.  Mid 2000’s: Geoffrey Hinton trains a deep network by:   Stacking  Restricted Boltzmann Machines (RBM’s) on  top of one another – deep belief network  Training layer by layer on un-labeled data  Then, using back propagation to fine tune weights on  labeled data Autoencoders 2.  2006: Yoshua Bengio et al. does something similar using auto- encoders instead of RBM’s  Both are 2 layer neural networks that learn to model  their inputs 30 BreakPage Autoencoder  The network is trained  to output the input x x 1 1  i.e. learn the identity  x x function.  2 2 a 1 x x  3 3 Trivial… unless, we  a impose constraints: 2 x x 4 4  Nb of units in layer 2 < nb  a of input units (learn  3 x x 5 5 compressed  representation)  +1 x x OR 6 6  Constrain layer 2 to be  sparse (i.e. many  layer 2 layer 3  +1 connections are  (output) layer 1  “disabled”)   (input) → Worksheet #6 (“Autoencoder”) 31 BreakPage Autoencoder Feedforward input x x 1 1 x x 2 2 a 1 x x 3 3 a 2 x x 4 4 a 3 x x 5 5 +1 x x 6 6 layer 2 output  +1 layer → Worksheet #6  input  Backprop error (“Autoencoder Activation”) layer 32 BreakPage Autoencoder x x 1 1 x x 2 2 a 1 x x 3 3 a 2 x x 4 4 a 3 x x 5 5 +1 x x 6 6 layer 2 output  +1 layer input  layer 33 BreakPage Autoencoder x 1 x 2 a 1   x 3 a 2 x 4 a 3 x 5 layer 2 x 6 New (compressed) representation of  +1 the input to be fed to the next layer i.e. the encoded x   input layer 34 BreakPage Autoencoder x 1 Feedforward input x 2 a b a 1 1 1 x 3 a b a 2 2 2 x 4 a b a 3 3 3 x 5 new output +1 +1  layer x 6 new input  Backprop error layer Train parameters (weights) +1 But keep b ’s sparse (ie. many  i zeros).  35 BreakPage Autoencoder x 1 x 2 a b 1 1 a 1 x 3 a b a 2 2 2 x 4 a b a 3 3 3 x 5 new output +1 +1  layer x 6 new input  layer +1 36 BreakPage Autoencoder x 1 x 2 a b 1 1   x 3 a b 2 2 x 4 a b 3 3 x 5 +1 +1 x 6 New representation for the  input to the next layer i.e. the encoded a +1 37 BreakPage Autoencoder x 1 Feedforward input x 2 a b c 1 b 1 1 1 x 3 a b c b 2 2 2 2 x 4 a b c b 3 3 3 3 x 5 new output  +1 +1 +1 layer x 6 Backprop error new input  Train parameters layer +1 subject to c ’s being sparse.  i 38 BreakPage Autoencoder x 1 x 2 a b c 1 1 1 b 1 x 3 b a 2 c b x 2 2 2 4 b a 3 c b 3 3 3 x 5 new output  +1 +1 +1 layer x 6 new input  layer +1 39 BreakPage Autoencoder x 1 x 2 a b c 1 1 1   x 3 a b c 2 2 2 x 4 a b c 3 3 3 x 5 New representation  +1 +1 +1 x for input.  6 Use [c , c , c ] as representation to  1 3 3 +1 feed to supervised learning algorithm  (the last, supervised, layer). 40 BreakPage Training a Deep NN train this  then this  then this  then this  finally  layer first layer layer layer this layer Use pretrained  Use of unlabelled data to “pretrain” the network representations to feed  i.e. learn more and more abstract feature  a regular ANN with a  representations smaller set of labelled  data.  41 BreakPage Many Types of Neural Networks    https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b46432 BreakPage Many Types of Deep Networks (con’t)  https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 43 BreakPage Today 1. Motivation 2. Feature Learning 3. Training a Deep Neural Network 4. CNNs for Image Processing 5. Conclusion 44 BreakPage CNNs for Image Processing CNNs = Convolutional Neural Networks Image of a 4 in grey scale Value = 0-> white …. 255->black https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 45 BreakPage CNNs for Image Processing  Standard input of the image in the ANN: 1 pixel = 1 input.  1. Eg. color image of 200x200x 3channels (RGB)  2.        --> in a fully connected ANN, a neuron of the                  input layer has 200*200*3 = 120,000 weights        --> huge number of parameters, can easily              overfit We linearize the image ==> We lose spatial  2. information … https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 46 BreakPage Convolutional Layer  Use a filter (aka kernel) that “convolves” on the image  Filter = small weight matrix to learn 86 4 8 184 219.2 23.9 147.2 252 3 8 40 283.9 22.9 349.5 1 0.3 34 7 7 163 0.5 2 92.6 16.1 195.4 105 2 3 69 139.6 20.4 377.7 56 3 8 175 121.9 9.9 417.5 126 1 2 178 223.8 13.6 341.4 163 8 4 142 620.4 268.2 443.6 22 222 74 180 486.1 731.2 736 163 158 204 253     47 BreakPage Convolutional Layer  Use a filter (aka kernel) that “convolves” on the image  Filter = small weight matrix to learn 86 4 8 184 219.2 23.9 147.2 252 3 8 40 283.9 22.9 349.5 1 0.3 34 7 7 163 0.5 2 92.6 16.1 195.4 105 2 3 69 139.6 20.4 377.7 56 3 8 175 121.9 9.9 417.5 126 1 2 178 223.8 13.6 341.4 163 8 4 142 620.4 268.2 443.6 22 222 74 180 486.1 731.2 736 163 158 204 253     48 BreakPage Convolutional Layer  Use a filter (aka kernel) that “convolves” on the image  Filter = small weight matrix to learn 86 4 8 184 219.2 23.9 147.2 252 3 8 40 283.9 22.9 349.5 1 0.3 34 7 7 163 0.5 2 92.6 16.1 195.4 105 2 3 69 139.6 20.4 377.7 56 3 8 175 121.9 9.9 417.5 126 1 2 178 223.8 13.6 341.4 163 8 4 142 620.4 268.2 443.6 22 222 74 180 486.1 731.2 736 163 158 204 253     49 BreakPage Convolutional Layer  Use a filter (aka kernel) that “convolves” on the image  Filter = small weight matrix to learn 86 4 8 184 219.2 23.9 147.2 252 3 8 40 283.9 22.9 349.5 1 0.3 34 7 7 163 0.5 2 92.6 16.1 195.4 105 2 3 69 139.6 20.4 377.7 56 3 8 175 121.9 9.9 417.5 126 1 2 178 223.8 13.6 341.4 163 8 4 142 620.4 268.2 443.6 22 222 74 180 486.1 731.2 736 163 158 204 253     → Worksheet #6 (“Activation Map”) 50 BreakPage Learn the Filters 18 54 51 239 244 188 429 505 686 856 1 0 1 55 121 75 78 95 88 0 1 0 261 792 412 640 35 24 204 113 109 221 1 0 1 633 653 851 751 3 154 104 235 25 130 608 913 713 657 15 253 225 159 78 233 68 85 180 214 245 0    The weight matrix (filter/kernel) behaves like a filter   The network learns the values of the filter(s) that activate when they  “see” some visual feature that is useful to identify the object (the final  classification)  Ex. a horizontal line, a blotch of some color, a circle…  https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 51 BreakPage Convolution Hyper-parameters 1. Stride 2. Padding 52 BreakPage Stride  (7×7)          W (3×3) with stride =1          C  (5×5)    (7×7)          W (3×3) with stride =2          C  (3×3)   53 BreakPage Padding 9 0 0 1 0 0 0 0 1 9 not picked up ;-(  1 0 1 0 0 1 0 1 1 0 1 1 2 0 0 0 filter should pick up high values surrounded by low values  2 1 0 1 0 0 0 0 0 0 0 0 0 9 0 0 1 0 9 0 0 1 0 0 1 0 1 0 1 0 9 picked up ;-)  0 1 0 1 0 0 0 0 0 0 1 1 2 0 0 1 1 2 0 2 1 0 1 0 2 1 0 1 0 0 0 0 0 0 0 54 BreakPage Learn Several Filters  So we create 1 activation map per filter 5x5 https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 55 BreakPage Pooling Layer  Used to:  To reduce the size of the activation maps  So that we reduce the number of parameters of the  network and hence avoid overfitting.  Several strategies:  Max pooling 429 505 686 856 792 856 261 792 412 640 913 851 633 653 851 751 608 913 713 657  Average pooling 429 505 686 856 496.8 648.5  … 261 792 412 640 701.8 743 633 653 851 751 608 913 713 657 → Worksheet #6 (“Pooling Layer”) https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 56 BreakPage Architecture of a CNN  Stack:  Convolutional Layers  Pooling Layers  Finish off with:  A fully connected layer at the end for the final classification https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 57 BreakPage Learning a Hierarchy of Features https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 58 BreakPage Example of a CNN http://cs231n.github.io/convolutional-networks/ 59 BreakPage Successful CNN Networks  LeNet  First successful applications of CNNs   Developed by Yann LeCun in the 1990’s  used to read zip codes, digits, etc.  AlexNet  First work that popularized CNNs for  computer vision  developed by Alex Krizhevsky, Ilya  Sutskever and Geoff Hinton (U. of Toronto)  In 2012 significantly outperformed all teams  at the ImageNet ILSVRC challenge http://cs231n.github.io/convolutional-networks/ 60 BreakPage Why now? 1. Basic science  Backpropagation did not work / overfitting…   now: developed method for training, better activation functions,  better architectures….   Need for lots training data…   now: we have massive amounts + unsupervised pre-training 2. GPU computing  Neural networks take very very long to train… (days, weeks)   now: use of GPU’s which are optimized for very fast matrix  multiplication 3. Open Access to resources  now : Access to DL methods, code and frameworks  now : Fast turnaround from idea to implementation 61 BreakPage History of AI Rules written by experts  (eg. linguistics, medical doctors,…) https:// 62 www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111 http://www.cormix.info/images/RuleTreeExample.jpg BreakPage History of AI Rules learns via the data ;-) But: features identified by the experts  (eg. linguistics, medical doctors,…)  63 BreakPage History of AI Rules AND  features  learned from the data 64 BreakPage Conclusion  Deep Learning is thriving !  vision  image processing  speech recognition  natural language processing  …  Canada is a world leader in Deep Learning Montreal: (Bengio et al.)   MILA 1. Toronto: (Hinton et al.)  Vector Institute 2. Edmonton: (Sutton et al.) AMII  3. 65 BreakPage 