




































COMP 6721: AI


Artificial Intelligence: 
Deep Learning
many slides from:  Y. Bengio, A. Ng and Y. LeCun

1



Today
1. Motivation
2. Feature Learning
3. Training a Deep Neural Network
4. CNNs for Image Processing
5. Conclusion

2



History of AI

3



Deep Learning in the Academic Press (2012-2015)

4



Deep Learning in the News (2013)

Slide from Yoshua Bengio, 2015 5



Deep Learning in the News (2012-2014)

Slide from Yoshua Bengio, 2015 6



Major Breakthroughs 
 Speech Recognition & Machine Translation (2010+)

 Skype Translator

 Image Recognition & Computer Vision (2012+)
 Natural Language Processing (2014+) 
 …

Figure from Yoshua Bengio, 2015

Google Translate

Skype Translator

Google now

7



Major Breakthroughs 
 Speech Recognition & Machine Translation (2010+) 
 Image Recognition & Computer Vision (2012+)

 Natural Language Processing (2014+)
 …

Figure from Yoshua Bengio, 2015

3.567%
3.581%

4th year

Object recognition Self driving cars

8



Major Breakthroughs 

Image Captioning (deep vision + deep NLP)

Question Answering

 Speech Recognition & Machine Translation (2010+)
 Image Recognition & Computer Vision (2012+)
 Natural Language Processing (2014+)

9



A

B

Image Captioning: Better than humans?

10



Today
1. Motivation
2. Feature Learning
3. Training a Deep Neural Network
4. CNNs for Image Processing
5. Deep Learning for NLP
6. Conclusion

11



A Deep Neural Net

https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c
https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/

The Google “Inception” deep neural network architecture for image recognition 
(27 layers) 

12

https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c
https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c


Initial Drawbacks
1. Standard backpropagation with sigmoid activation function 

does not scale well with multiple layers
 Weight of early layers change too slowly (no learning)

2. Overfitting
 Large network -> lots of parameters -> increased capacity to 

“learn by heart”
3. Multilayered ANNs need lots of labeled data 

 Most data is not labeled :(

13



Initial Drawbacks (1)
1. Standard gradient-based backpropagation does not scale well with 

multiple layers…

When we multiply the gradients many times (for 
each layer),  it can lead to …

a) Vanishing gradient problem: 
 gradients shrink exponentially with the 

number of layers 
 so weight updates get smaller and smaller
 and weights of early layers change very 

slowly and network learns very very slowly
b) Exploding gradient problem:

 multiplying gradients could also make them 
grow exponentially.  

 so weight updates get larger and larger
 and the weights can become so large as to 

overflow and result in NaN values

14



Initial Drawbacks (1)

https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f

https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6

To help, we can :
a) Use other activation 

functions…

b) Do “gradient clipping” 
(i.e. set bounds on the 
gradients)

15

https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6


Initial Drawbacks (2)

https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html

 

2. Overfitting
 Large network -> lots of parameters -> 

increased capacity to “learn by heart”
 Solutions:

 Regularization: 
 modify the error function that we minimize to penalize large weights.

 where f(w) grows larger as the weights grow larger and λ is the 
regularization strength

 Dropout: 
 keep a neuron active with some probability p or setting it to 

zero otherwise. 
 prevents the network from becoming too dependent on any 

one neuron. 

16



Initial Drawbacks (3)
3. Multilayered ANNs need lots of labeled data 

 Solution: “pre-train” the network with features found 
automatically using unsupervised data 

 i.e. Automatic feature learning… (see next few slides)

17



Classic ML

Input

Motorbikes
“Non”-Motorbikes

Learning
algorithm

Feature 1

Fe
at

ur
e 

2

Manual Extraction of Features 
(eg. edge detection, colors, 

texture,…)

Classic ML, 
requires labeled 
data and hand-
crafted features

1. Needs expert 
knowledge

2. Time-consuming 
     and expensive

3. Does not     
    generalize to 
    other domains

Slide from Y. LeCun
18



Automatic Feature Learning

Input

Motorbikes
“Non”-Motorbikes

Automatic 
Feature 

Representation
Learning
algorithm

“wheel”

“h
an

dl
e”

handle

wheel
eg. handle, wheel, … With Automatic 

Feature Learning:

1. We feed the 
network the raw data 
(not feature-
curated)

2. The features are 
learned by the 
network

3. Features learned 
can be re-used in 
similar tasks. 

19
Slide from Y. LeCun



Automatic Feature Learning

https://www.strong.io/blog-images/movie-posters/Slide6.png 20



Automatic Feature Learning

 Each layer learns more abstract features that are then combined / 
composed into higher-level features automatically

 Like the human brain … 
 has many layers of neurons which act as feature detectors
 detecting more and more abstract features as you go up

 E.g. to classify an image of a cat:
 Bottom Layers: Edge detectors, curves, corners straight lines
 Middle Layers: Fur patterns, eyes, ears
 Higher Layers: Body, head, legs
 Top Layer: Cat or Dog

Deep Learning = Machine learning algorithms based on 
learning multiple levels of representation / abstraction. 
– Y. Bengio

21



Automatic Feature Learning

22



What Types of Features?
 For image recognition 

 pixel -> edge -> texton -> motif -> part -> object

 For NLP
 character -> word ->  constituents -> clause -> sentence -> discourse

 For speech:
 sample  spectral band -> sound -> … phone -> phoneme -> word→

Figure from Y LeCun 23



Eg: Learning Image Features
Faces Cars Elephants Chairs

Actual images 
(pixels)

24

Learned 
object 
parts

Learned 
edges

Examples of learned objects parts from object categories

Learned 
objects

Learned features / 
representations can be used in  
variety of OTHER classification 

tasks…  deep learning→



Advantages of Unsupervised Feature 
Learning 

25



Advantages of Unsupervised 
Feature Learning 
 Much more unlabeled data available than labeled data:

 Eg. Websites, Books, Videos, Pictures
 Humans learn initially from unlabeled examples

 Eg. Babies learn to talk/recognize objects without labeled data
 As the features are learned in an unsupervised way from a 

different and larger dataset, less risk of over-fitting
 No need for manual feature engineering 
 These features are organized into multiple levels

 Each level creates new features from combinations of features from 
the level below

 Each level is more abstract than the ones below (hierarchy of 
features)

26



Today
1. Motivation
2. Feature Learning
3. Training a Deep Neural Network
4. CNNs for Image Processing
5. Deep Learning for NLP
6. Conclusion

27



General Architecture of a Deep Network

1. Unsupervised pre-training of neural network using 
unlabeled data
 aka. unsupervised learning of features

2. Supervised training with labeled data using features 
learned from above with a standard classifier
 Eg. an ANN, SVM, … 

28



Training a Deep NN

then this 
layer

finally 
this layer

then this 
layer

then this 
layer

train this 
layer first

EACH of the (non-output) layers is trained to 
learn a representation of the data (eg. 
autoencoder) aka “pretraining the network 
with unsupervised data”

Use pretrained 
representations to feed a 
regular ANN with a smaller 
set of labelled data. 

29



Autoencoders
 To learn a representation of the data, we can use:

1. Deep Belief Networks 
 Mid 2000’s: Geoffrey Hinton trains a deep network by: 

 Stacking  Restricted Boltzmann Machines (RBM’s) on 
top of one another – deep belief network

 Training layer by layer on un-labeled data
 Then, using back propagation to fine tune weights on 

labeled data

2. Autoencoders
 2006: Yoshua Bengio et al. does something similar using auto-

encoders instead of RBM’s

 Both are 2 layer neural networks that learn to model 
their inputs

30



Autoencoder
 The network is trained 

to output the input
 i.e. learn the identity 

function. 

 Trivial… unless, we 
impose constraints:

 Nb of units in layer 2 < nb 
of input units (learn 
compressed 
representation) 

OR
 Constrain layer 2 to be 

sparse (i.e. many 
connections are 
“disabled”)  

x4

x5

x6

+1

layer 1 
(input)

layer 2

x1

x2

x3

x4

x5

x6

x1

x2

x3

+1

layer 3 
(output)

a1

a2

a3

31 → Worksheet #6 (“Autoencoder”)



Autoencoder

x4

x5

x6

+1

input 
layer

layer 2

x1

x2

x3

x4

x5

x6

x1

x2

x3

+1

a1

a2

a3

Feedforward input

Backprop error

output 
layer

32

 → Worksheet #6 
(“Autoencoder Activation”)



Autoencoder

x4

x5

x6

+1

input 
layer

layer 2

x1

x2

x3

x4

x5

x6

x1

x2

x3

+1

a1

a2

a3

output 
layer

33



Autoencoder

New (compressed) representation of 
the input to be fed to the next layer

i.e. the encoded x 

x4

x5

x6

+1

layer 2

x1

x2

x3

a1

a2

a3

 input layer

 

34



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

Train parameters (weights)
But keep bi’s sparse (ie. many 
zeros). 

Autoencoder

new input 
layer

new output
 layer

Feedforward input

Backprop error

a1

a2

a3

35



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

Autoencoder

new input 
layer

new output
 layer

a1

a2

a3

36



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

Autoencoder

New representation for the 
input to the next layer

i.e. the encoded a

 

37



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

+1

c1

c2

c3

Autoencoder

new input 
layer

new output 
layer

Train parameters
subject to ci’s being sparse. 

Feedforward input

Backprop error

b1

b3

b2

38



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

+1

c1

c2

c3

Autoencoder

new input 
layer

new output 
layer

b1

b2

b3

39



x4

x5

x6

+1

x1

x2

x3

+1

a1

a2

a3

+1

b1

b2

b3

+1

c1

c2

c3

New representation 
for input. 

Use [c1, c3, c3] as representation to 
feed to supervised learning algorithm 

(the last, supervised, layer).

Autoencoder

 

40



Training a Deep NN

then this 
layer

finally 
this layer

then this 
layer

then this 
layer

train this 
layer first

Use of unlabelled data to “pretrain” the network
i.e. learn more and more abstract feature 
representations

Use pretrained 
representations to feed 
a regular ANN with a 
smaller set of labelled 
data. 

41



Many Types of Neural Networks 
 

https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b46342



Many Types of Deep Networks (con’t) 

https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 43



Today
1. Motivation
2. Feature Learning
3. Training a Deep Neural Network
4. CNNs for Image Processing
5. Conclusion

44



CNNs for Image Processing

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

Image of a 4 in grey scale
Value = 0-> white …. 255->black

CNNs = Convolutional Neural Networks

45



CNNs for Image Processing

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 Standard input of the image in the ANN:
1. 1 pixel = 1 input. 
2. Eg. color image of 200x200x 3channels (RGB) 
       --> in a fully connected ANN, a neuron of the     
            input layer has 200*200*3 = 120,000 weights
       --> huge number of parameters, can easily
             overfit
2. We linearize the image ==> We lose spatial 

information

…

46



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

47



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

48



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

49



Convolutional Layer
 Use a filter (aka kernel) that “convolves” on the image
 Filter = small weight matrix to learn

 
 

1 0.3
0.5 2

219.2 23.9 147.2
283.9 22.9 349.5

92.6 16.1 195.4
139.6 20.4 377.7
121.9 9.9 417.5

223.8 13.6 341.4
620.4 268.2 443.6
486.1 731.2 736

86 4 8 184
252 3 8 40

34 7 7 163
105 2 3 69
56 3 8 175

126 1 2 178
163 8 4 142
22 222 74 180

163 158 204 253

50

 → Worksheet #6 (“Activation Map”)



Learn the Filters

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 The weight matrix (filter/kernel) behaves like a filter 
 The network learns the values of the filter(s) that activate when they 

“see” some visual feature that is useful to identify the object (the final 
classification)
 Ex. a horizontal line, a blotch of some color, a circle… 

 

429 505 686 856
261 792 412 640
633 653 851 751
608 913 713 657

1 0 1
0 1 0
1 0 1

18 54 51 239 244 188
55 121 75 78 95 88
35 24 204 113 109 221

3 154 104 235 25 130
15 253 225 159 78 233
68 85 180 214 245 0

51



Convolution Hyper-parameters

1. Stride
2. Padding

52



Stride

 (7×7)          W (3×3) with stride =1          C  (5×5)  

 (7×7)          W (3×3) with stride =2          C  (3×3)  

53



Padding
9 0 0 1
1 0 1 0
0 1 1 2
2 1 0 1

0 0 0
0 1 0
0 0 0

0 1
1 1

filter should pick up high values surrounded by low values 

9 not picked up ;-( 

0 0 0 0 0 0
0 9 0 0 1 0
0 1 0 1 0 0
0 0 1 1 2 0
0 2 1 0 1 0
0 0 0 0 0 0

0 0 0
0 1 0
0 0 0

9 0 0 1
1 0 1 0
0 1 1 2
2 1 0 1

9 picked up ;-) 

54



Learn Several Filters

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 So we create 1 activation map per filter

5x5

55



Pooling Layer

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 Used to:
 To reduce the size of the activation maps
 So that we reduce the number of parameters of the 

network and hence avoid overfitting.
 Several strategies:

 Max pooling

 Average pooling
 …

429 505 686 856
261 792 412 640
633 653 851 751
608 913 713 657

792 856
913 851

429 505 686 856
261 792 412 640
633 653 851 751
608 913 713 657

496.8 648.5
701.8 743

56

 → Worksheet #6 (“Pooling Layer”)



Architecture of a CNN

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/

 Stack:
 Convolutional Layers
 Pooling Layers

 Finish off with:
 A fully connected layer at the end for the final classification

57



Learning a Hierarchy of Features

https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/
58



Example of a CNN

http://cs231n.github.io/convolutional-networks/
59



Successful CNN Networks

http://cs231n.github.io/convolutional-networks/

 LeNet
 First successful applications of CNNs 
 Developed by Yann LeCun in the 1990’s
 used to read zip codes, digits, etc.

 AlexNet
 First work that popularized CNNs for 

computer vision
 developed by Alex Krizhevsky, Ilya 

Sutskever and Geoff Hinton (U. of Toronto)
 In 2012 significantly outperformed all teams 

at the ImageNet ILSVRC challenge

60

http://yann.lecun.com/exdb/lenet/
http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf


Why now?
1. Basic science

 Backpropagation did not work / overfitting… 
 now: developed method for training, better activation functions, 

better architectures…. 
 Need for lots training data… 
 now: we have massive amounts + unsupervised pre-training

2. GPU computing
 Neural networks take very very long to train… (days, weeks) 
 now: use of GPU’s which are optimized for very fast matrix 

multiplication
3. Open Access to resources

 now : Access to DL methods, code and frameworks
 now : Fast turnaround from idea to implementation

61



History of AI

https://
www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
http://www.cormix.info/images/RuleTreeExample.jpg

Rules written by experts 
(eg. linguistics, medical doctors,…)

62

https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111


History of AI Rules learns via the data ;-)
But: features identified by the experts 
(eg. linguistics, medical doctors,…) 

63



History of AI
Rules AND 
features 
learned from the data

64



Conclusion
 Deep Learning is thriving !

 vision
 image processing
 speech recognition
 natural language processing
 …

 Canada is a world leader in Deep Learning
1. Montreal: (Bengio et al.)   MILA
2. Toronto: (Hinton et al.)  Vector Institute
3. Edmonton: (Sutton et al.) AMII 

65

https://mila.quebec/en/
http://vectorinstitute.ai/
https://www.amii.ca/

	Slide 1
	Slide 2
	Slide 3
	Slide 4
	Slide 5
	Slide 6
	Slide 7
	Slide 8
	Slide 9
	Slide 10
	Slide 11
	Slide 12
	Slide 13
	Slide 14
	Slide 15
	Slide 16
	Slide 17
	Slide 18
	Slide 19
	Slide 20
	Slide 21
	Slide 22
	Slide 23
	Slide 24
	Slide 25
	Slide 26
	Slide 27
	Slide 28
	Slide 29
	Slide 30
	Slide 31
	Slide 32
	Slide 33
	Slide 34
	Slide 35
	Slide 36
	Slide 37
	Slide 38
	Slide 39
	Slide 40
	Slide 41
	Slide 42
	Slide 43
	Slide 44
	Slide 45
	Slide 46
	Slide 47
	Slide 48
	Slide 49
	Slide 50
	Slide 51
	Slide 52
	Slide 53
	Slide 54
	Slide 55
	Slide 56
	Slide 57
	Slide 58
	Slide 59
	Slide 60
	Slide 61
	Slide 62
	Slide 63
	Slide 64
	Slide 65

